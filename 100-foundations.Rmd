# Foundations {#foundations}


In this chapter:

* The data preparation process

* The principles of data quality, and how dirty data is part of data quality

* Understanding what makes data "clean" or “dirty”, and why context matters

* Understanding tidy data principles



## The data preparation process

As we get started on the path of preparing our data, we should think first about where we want to go.

Ellis and Leek's article "How to Share Data for Collaboration" [@Ellis_Leek_2017] is aimed at scientists ("data generators") who have collected the data and are preparing it for further analysis in an academic environment—the authors speak about "preparing data for a statistician". 

While Ellis and Leek have a specific context in mind, the principles in their paper have broad applicability, and the practices are  essential in any environment, including a business, government, or non-profit organization. And in those contexts, the roles and responsibilities, and the division of labour in the workflow, are often different than in an academic environment. The data collection and storage might be to support a business need, and the use of the data for business intelligence is a secondary benefit. As a result, there is often a middle person who isn't involved in the collection of the data, but who does the preparation of the data for the modelling, visualization, and reporting. That same person might also be responsible for the modelling, visualization, and reporting.

At the end of the data preparation phase, before visualization and modelling can start, there should be:

**1. The raw data.**

**2. A tidy dataset [@tidydata]**

**3. A code book describing each variable and its values in the tidy dataset.**

**4. An explicit and exact recipe used by the researcher to go from 1 to 2 to 3.**

Over the next few chapters in this book, we will look at examples of creating a tidy data set (including cleaning the data), creating the code book and documenting our steps along the way, thereby creating an "explicit and exact recipe".

Whether we are preparing data as part of our own analytic project or to make it available for sharing, these should be our goals.

### Elements of an iterative process

This list below are things that make up the process you will go through in preparing your data for analysis and modelling. These are not "steps", in that data preparation is an iterative process, it's not linear or even necessarily sequential.

1. Save a copy of the original data. Do not make any edits, and consider making the file "read-only".

2. Start a new blank "readme" file, and in that file record a few key points, including the project's research objective and information about the data file's origin. You can also draft an outline of the steps that you think you're going to take in the preparation process, as the first contribution to a literate programming approach.

3. Import the data into your **R** environment. 

4. Explore and validate the data, assessing the structure, looking for missing values and other "dirty" elements.

5. Clean the data. (And then validate to make sure your cleaning has been successful...)

This whole process is iterative (you will be adding to the readme file at every step), and the individual steps themselves are iterative. For example, your first code to import the data may be revisited, as you evaluate and assign variable types through the various arguments in the read function. Or your first cleaning code may address one dirty element, only to expose a second that you hadn't previously identified.

In addition, you will be using your data wrangling, manipulation, and visualization skills at various points along the way.



## Data quality

Part of the data preparation process is to ask "What is the data's _quality_?" 

One resource that is useful to frame your thinking on this is Statistics Canada's _Quality Guidelines_ [@StatCan_data_quality_6]. Like other official statistical agencies around the world, Statistics Canada's reputation is staked on making high quality data available to the widest possible audience. They have identified six elements of data quality.

* Relevance: The degree to which the data meets the user's needs, and relates to the issues that the user cares about.

* Timeliness and punctuality: The delay between the information reference period and the date when the data becomes available.

* Accuracy and reliability: Accuracy "is the degree to which the information correctly describes the phenomena it was designed to measure."

* Accessibility and clarity: "The ease with which users can learn that the information (including metadata) exists, find it, view it and import it into their own work environment."

* Interpretability: "The availability of supplementary information and metadata needed to interpret and use statistical information appropriately."

* Coherence and comparability: "The degree to which it can be reliably combined and compared with other statistical information within a broad analytical framework over time."

In our data preparation process, we want to ensure our own work is accessible and interpretable; this is the motivation behind documentation. 

Another typology of "data quality" is found in [@WangRichardY.1995TqdA]. High quality data is:

* Accessible

* Interpretable

* Useful

* Believable


"Dirty data" is data that falls short on the _believable_ dimension, in particular evaluating whether the data are complete, consistent, and accurate. It is important to note that these categories are not mutually exclusive; a variable might be simultaneously inconsistent and inaccurate.

**Complete**

For our purposes, "complete" means whether any values in each record are missing (internally complete).

"Complete" does not mean that every possible member of the population is represented in the data. With a well-designed sample, it is possible that a sub-set of the population can provide an accurate set of measures about the population. Furthermore, it is possible to determine whether the records are an accurate representation of the whole.

**Consistent**

We will consider a measure to be "consistent" if the same value is reported in the same way.

Some examples of consistency:

* Units are consistent. One example is temperature, and ensuring that the values are consistently reporting in degrees Celsius, not mixing Fahrenheit and Kelvin. Another might be in a survey or form with an international audience, where `salary` might be completed by respondents in the values of their local currency.

* Spelling is consistent. My home province in Canada is "British Columbia", but is often abbreviated to "B.C." or "BC". Or consider the 57 different ways that "Philadelphia" was spelled in the US Paycheck Protection Program (PPP) applications. [@Au_data_cleaning_2020]

* In some cases, the mode of data collection can introduce differences in the value recorded. This can apply to everything from variability of scientific instruments such as air quality sensors [@Khreis_etal_2022] to how people respond to surveys conducted in different media [@Abeysundera_2015] [@Holbrook_Green_Krosnick_2003] [@St-Pierre_Beland_2004].



**Accurate**

When we say "accurate", we mean that the value recorded in our data is the value measured.

Some examples of inaccurate data are:

* The use of a default (or sentinel) value\index{sentinel value}, inserted as a placeholder for unknown or missing values in place of a missing or "unknown" value. This is sometimes part of the data collection or database software, leading to values that should have been "unknown" be entered as the default. This can be the consequence of a data entry validation process that requires an entry (that is, data entry cannot continue until the field has an entry) or an entry in a particular format, such as a date. In these cases, the sentinel value can be entered instead of an explicit "NA".

> Dr Davis Lawrence, director of safety-literature database the SafeLit Foundation...tells me that 'in most US states the quality of police crash reports is at best poor for use as a research tool. ... Data-quality checks were rare and when quality was evaluated it was found wanting. For example, in Louisiana for most crashes in the 1980s most of the occupants were males who were born on January 1st, 1950. Almost all of the vehicles involved in crashes were the 1960 model year.' Except they weren't. These were just the default settings. [@Criado_Perez_2019, p.190]



* A data entry error. Perhaps the most common are typographical errors, where the wrong value is entered. Another type is what we might call a "variable transposition error" (where the value is transposed one column over, something that happens all too often with address records, where the city name might end up in the state/province column).

* Values are transformed automagically by software. Microsoft's Excel spreadsheet program is the most famous of these, converting many non-date values into date format, and assigning new values to store the data. For example, entering the character string "SEPT1" gets converted to September 1st of the current year. There is documented evidence that this software behaviour has caused errors in gene research. [@Zeeberg_etal_2004], [@Abeysooriya_etal_2021]


* Dates may be stored as a numeric value (or "serial number") representing the number of days elapsed from a fixed starting point—a starting point that varies by operating system. For example, Excel for Windows uses January 1, 1900 as the first day, while earlier versions Macintosh computers, and by extension Excel for Macintosh, used January 1, 1904 as the start date. Thus the same date would be stored as different values. ^["Differences between the 1900 and the 1904 date system in Excel"](https://docs.microsoft.com/en-us/office/troubleshoot/excel/1900-and-1904-date-system)


* Contradiction. For example, imagine that a single individual is entered into two databases, but when we compare them we see that the date of birth differs, perhaps due to non-ISO8601 entry: 07-08-79 and 08-07-79 both have the same digits but one could be mm-dd-yy and the other dd-mm-yy...we just don't know which is the correct one. Or is one a typo?


* Cultural ignorance (for want of a better term).

> "Prawo Jazdy"\index{Prawo Jazdy} was a supposed Polish national who was listed by the Garda Síochána in a police criminal database as having committed more than 50 traffic violations in Ireland. A 2007 memorandum stated that an investigation revealed prawo jazdy [ˈpra.vɔ ˈjaz.dɨ] to be Polish for 'driving licence', with the error arising due to officers mistaking the phrase, printed on Polish driving licences, to be a personal name while issuing traffic tickets. [@wiki:prawo_jazdy]


### Tidy data

When we are cleaning our data, we should also consider the structure. The goal should be a tidy structure \index{tidy data}, one that meets the following three principles or rules: 

* Each variable must have its own column.

* Each observation mus have its own row.

* Each value must have its own cell. [@tidydata]

In many instances, achieving a tidy structure may require reshaping the data, using the {tidyr}[@R-tidyr] packages's `pivot_()` functions. In others, the data may have multiple variables in each cell which need to be split (such as addresses) into a separate column for each variable.


Other common structural problems that fall outside the four broad data quality categories include:

1. Column headers are values instead of variable names. A frequently-encountered example are units of time, as in the example below. In the table below, the "values" are the years. Tables structured in this way can only display the values of one variable; in this case, it is the population of the country.



```{r gapminder_untidy, echo=FALSE}

gap_pop <- gapminder |> 
  filter(country %in% c("Canada", "United States")) |> 
  filter(year %in% c(1952, 2007)) |> 
  mutate(year = as.character(year),
         pop = as.integer(pop)) |> 
  select(country, year, "population" = pop)

gap_pop_untidy <- gap_pop |> 
  pivot_wider(id_cols = country, values_from = population, names_from = year) 

gap_pop_untidy |> 
  gt() |> 
  fmt_integer(
    columns = c("1952", "2007"),
    sep_mark = ","
    )

```

A preferable structure would be to have a variable "year" and another with "population". This tidy structure also permits other variables measured during those years to be incorporated, rather than displayed in a separate table.

```{r, echo=FALSE}
gap_pop_2 <- gapminder |> 
  filter(country %in% c("Canada", "United States")) |> 
  filter(year %in% c(1952, 2007)) |> 
  select(country, 
         year, 
         "population" = pop,
         "life expectancy" = lifeExp) |> 
  mutate(year = as.character(year))

gap_pop_2 |> 
  gt() |> 
  fmt_integer(
    columns = c("population"),
    sep_mark = ","
    )
  
```



2. Variable names that are duplicated (this is possible in some data storage formats, including plain text and Excel files).

3. Inappropriate data types, such as numbers or logical values stored as character strings.

4. Multiple data types in a column. For example, some records are numbers and some are character strings. The values themselves could be entirely correct, but if some number entry includes commas as thousands separators, the variable will be read and stored as a character type.




## Cleaning the data

If the data fails to meet our standards or quality, we need to _clean the data_. Which doesn't sound like a lot of fun. Didn't we want to be data scientists or business intelligence experts or academic scientists uncovering the insights hiding in the data? Don't we want to be _doing analysis_? 

There is a strong argument to be made that the process of cleaning data is a fundamental part of the analytic process, or the corresponding statement that any analytic process requires data cleaning.

Randy Au has written "The act of cleaning data imposes values/judgments/interpretations upon data intended to allow downstream analysis algorithms to function and give results. That’s exactly the same as doing data analysis. In fact, “cleaning” is just a spectrum of reusable data transformations on the path towards doing a full data analysis." [@Au_data_cleaning_2020]

At this point we should ask what are we doing when we say we are "cleaning the data"? And how can we confirm that it is "cleaned" in the way that we have defined?


**The first challenge**: How do we find the things that are problematic with our data?

**The second challenge**: What can and should we do about them?

We will see some of these challenges and solutions in the next few chapters, as part of the data import process. We will also return to explicitly address these challenges in the chapters \@ref(validation) and \@ref(cleaning).


<!-- 
This file by Martin Monkman is licensed under a Creative Commons Attribution 4.0 International License. 
-->


