# Foundations {#foundations}

In this chapter:

* What is the goal?

* What is data?

* Classification systems


## The goal

As we get started on the path of preparing our data, we should think first about where we want to go.

Ellis and Leek's article "How to Share Data for Collaboration" [@Ellis_Leek_2017] is aimed at scientists ("data generators") who have collected the data and are preparing it for further analysis in an academic environment—the authors speak about "preparing data for a statistician". 

While Ellis and Leek have a specific context in mind, the principles in their paper have broad applicability, and the practices are  essential in any environment, including a business, government, or non-profit organization. And in those contexts, the roles and responsibilities, and the division of labour in the workflow, are often different than in an academic environment. The data collection and storage might be to support a business need, and the use of the data for business intelligence is a secondary benefit. As a result, there is often a middle person who isn't involved in the collection of the data, but who does the preparation of the data for the modeling, visualization, and reporting. That same person might also be responsible for the modeling, visualization, and reporting.

At the end of the data preparation phase, before visualization and modeling can start, there should be:

**1. The raw data.**

**2. A tidy dataset [@tidydata]**

**3. A code book describing each variable and its values in the tidy dataset.**

**4. An explicit and exact recipe used by the researcher to go from 1 to 2 to 3.**

Over the next few chapters in this book, we will look at examples of creating a tidy data set (including cleaning the data), and then creating the code book...and documenting our steps along the way, thereby creating an "explicit and exact recipe".

Whether we are preparing data as part of our own analytic project or to make it available for sharing, these should be our goals.


## What is data?

Before we get started cleaning our data, we should step back and ask ourselves what we mean by "data". One definition of data is as follows:

> Characteristics or information, usually numerical, that are collected through observation. [@OECD_glossary_2007, p.180]

This can be expanded:

> The data obtained from observations are related to the variable being studied. These data are quantitative, qualitative, discrete or continuous if the corresponding variable is quantitative, qualitative, discrete or continuous, respectively. [@Dodge_encyclopedia_2008, "Data", pp.149-150]


Before we can turn these definitions into something we can put into our model, we have to attend to some nuance. In _Learning Statistics with r: A Tutorial for Psychology Students and Other Beginners_, Danielle Navarro writes:

> Here are four different things that are closely related to each other:
>
> **__A theoretical construct__**. This is the thing that you’re trying to take a measurement of, like “age”, “gender” or an “opinion”. A theoretical construct can’t be directly observed, and often they’re actually a bit vague.
>
>**__A measure__**. The measure refers to the method or the tool that you use to make your observations. A question in a survey, a behavioural observation or a brain scan could all count as a measure.
>
>**__An operationalisation__**. The term “operationalisation” refers to the logical connection between the measure and the theoretical construct, or to the process by which we try to derive a measure from a theoretical construct.
>
>**__A variable__**. Finally, a new term. A variable is what we end up with when we apply our measure to something in the world. That is, variables are the actual “data” that we end up with in our data sets. [@Navarro_learning_statistics_2019, pp.13–14]

Let's consider an example. A company wants to understand its workforce because there's a sense that a large number of the staff are thinking about retirement in the next couple of years. The term used by the human resource (HR) department is "succession planning", meaning developing existing staff and recruiting to prepare for departures. Before making the business decision to allocate any resources to this initiative, the company's executive has asked the HR department to describe the magnitude of the risk to the organization: What percent of the staff are about to retire? In what departments? What positions do these potential retirees occupy? Now the HR department has asked you, the data scientist, for an analysis of the demographics of the company. 

We know that _age_ is going to be a _theoretical construct_ we want to measure. When it comes to retirement, that is constrained to older workers, but we will have to be more precise in our definitions that simply "older". In Canada you can choose to start collecting your [Canada Pension Plan (CPP)](https://www.canada.ca/en/services/benefits/publicpensions/cpp.html) at age 60, so we might decide that our analysis will be to find the proportion of staff who are either already 60 or older (those who could collect their national pension now), and a second group who are between 55 and 59 (those who are within five years of their earliest pension).

A _measure_ we have available is in the human resource database: each staff member's date of birth. This is collected when people are hired, and is required for various purposes, including tax accounting.

But you will have already noticed that your date of birth is not your age. To find your age, we have to calculate the difference between that date and today. So to _operationalize_ the measure we have, we might need an **R** equation. Using the functions in the package {lubridate} [@R-lubridate], that looks like this:

`age_this_year <- ymd(19731022) %--% today() %/% years(1)` ^[The birthdate of Ichiro Suzuki.] ^[This code chunk was modified from [@Richmond_age_2018]. For more information about calculating intervals using the {lubridate} package; see [@Ford_lubridate_2017].]


And our _variable_ results from this calculation: in the code example we called it `age_this_year`. Note that this variable will require recalculation every time we repeat our analysis—one's age is not a static value! In a company with roughly 2,500 staff, there's 50-50 chance it is someone's birthday today, and with 4,000 staff it is virtually certain at least one person is celebrating a birthday. [@Yau_birthday_2017] And of course, staff join and leave an organization, so the individuals in our database will change. This means that the variable we calculate will be precisely correct for only a few days at most.



## Data quality


We also need to ask "What is the data's _quality_?" 

One resource that is useful to frame your thinking on this is Statistics Canada's _Quality Guidelines_ [@StatCan_data_quality_6]. Like other official statistical agencies around the world, Statistics Canada's reputation is staked on making high quality data available to the widest possible audience.

* Relevance: The degree to which the data meets the user's needs, and relates to the issues that the user cares about.

* Timeliness and punctuality: The delay between the information reference period and the date when the data becomes available.

* Accuracy and reliability: Accuracy "is the degree to which the information correctly describes the phenomena it was designed to measure."

* Accessibility and clarity: "The ease with which users can learn that the information (including metadata) exists, find it, view it and import it into their own work environment."

* Interpretability: "The availability of supplementary information and metadata needed to interpret and use statistical information appropriately."

* Coherence and comparability: "The degree to which it can be reliably combined and compared with other statistical information within a broad analytical framework over time."

We will return to the concepts of data quality in the context of data cleaning, and what makes data _dirty_. 



## Classification systems

It's useful to be aware of various classification systems that already exist. That way, you can: 

* save yourself the challenge of coming up with your own classifications for your variables, and

* ensure that your analysis can be compared to other results.

Of course, these classification systems are subject-matter specific. 

As one example, in much social and business data, people's ages are often grouped into 5-year "bins", starting with ages 0–4, 5–9, 10–14, and so on. But not all age classification systems follow this; it will be driven by the specific context. The demographics of a workforce will not require a "0–4" category, while an analysis of school-age children might group them by the ages they attend junior, middle, and secondary school. As a consequence, many demographic tables will provide different data sets with the ages grouped in a variety of ways. (For an example, see [@StatCan_age_of_person].)

And be aware that the elements with a classification system can change over time. One familiar example is that the geographical boundaries of cities and countries of the world have changed. City boundaries change, often with the more populous city expanding to absorb smaller communities within the larger legal entity. (The current City of Toronto is an amalgamation of what were six smaller municipalities.) Some regions have a rather turbulent history of changes in their boundaries, and some countries have been created only to disappear a few years later. 

If you're working in the area of social and economic research, the national and international statistics agencies provide robust and in-depth classification systems, much of which is explicitly designed to allow for international comparisons. 

The North American Industry Classification System (NAICS) is one such classification system, used in Canada, the United States, and Mexico. [@US_NAICS_2022, @StatCan_NAICS] The system is used to classify businesses into a specific industry based on their primary output. This consistency allows for reliable and comparable analysis of the economies of the three countries. 

Here are some other resources with further examples:

* International: United Nations Statistics Division [@UNSD_classifications]

* Statistics Canada [@StatCan_definitions]


Your subject area has established classifications; seek them out and use them. Ensuring that your data can be combined with and compared to other datasets will add value to your analysis, and make it easier for future researchers (including future you!) to work with your data. [@White_etal_nine_simple_ways]


<!-- 
This file by Martin Monkman is licensed under a Creative Commons Attribution 4.0 International License. 
-->


