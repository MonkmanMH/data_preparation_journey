---
output:
  pdf_document: default
  html_document: default
#  word_document: default
---


<!-- 
This file by Martin Monkman is licensed under a Creative Commons Attribution 4.0 International License. 
-->



```{r setup_ch700, include=FALSE}

# packages

source("package_load.R")

```


# Cleaning techniques {#cleaning}

In this chapter:


* Cleaning dates and strings

* Creating conditional and calculated variables

  - indicator variables [@Gelman_etal_2014, p.366] (also known as dummy variables [@Suits_1957] and "one hot" encoding)
  
  - normalization

* Dealing with missing values



## Cleaning dates

A recurring challenge in many data analysis projects is to deal with date and time fields. One of the first steps of any data cleaning project should be to ensure that these are in a consistent format that can be easily manipulated.

As we saw in [@Broman_Woo_2017], there is an international standard (ISO 8601)\index{ISO 8601} for an unambiguous and principled way to write dates and times (and in combination, date-times). For dates, we write **YYYY-MM-DD**. Similarly, time is written **T[hh][mm][ss]** or **T[hh]:[mm]:[ss]**, with hours (on a 24-hour clock) followed by minutes and seconds.

When we are working with dates, the objective of our cleaning is to transform the date into the ISO 8601 standard. The benefits of this format are many, not the least of which is that there are R tools built for us to streamline analysis and visualization using this structure, and those same tools allow us to tackle any cleaning. 

We will work with {lubridate}, part of the tidyverse.

### Creating a date-time field

The first way that dates might be represented in your data is as three separate fields, one for year, one for month, and one for day. These can be assembled into a single <date> variable using the `make_date()`, or a single <datetime> variable using `make_datetime()`.

In the {Lahman}[@R-Lahman] table "People", each player's birthdate is stored as three separate variables. They can be combined into a single variable using `make_date()`:

```{r}
Lahman::People %>% 
  slice_head(n = 10) %>% 
  select(playerID, birthYear, birthMonth, birthMonth, birthDay) %>% 
  # create date-time variable
   mutate(birthDate = make_datetime(birthYear, birthMonth, birthDay)) 

```


The second problematic way dates are stored is in a variety of often-ambiguous sequences. This might included text strings for the month (for example "Jan", "January", or "Janvier" rather than "01"), two-digit years (does "12" refer to 2012 or 1912?), and ambiguous ordering of months and days (does "12" refer to December or the twelfth day of the month?)

If we know the way the data is stored, we can use {lubridate} functions to process the data into YYYY-MM-DD. These three very different date formats can be transformed into a consistent format:

```{r}
ymd("1973-10-22")
mdy("October 22nd, 1973")
dmy("22-Oct-1973")
```


A third manner in which dates can be stored is with a truncated representation. For example, to total sales for the entire month might be recorded as a single value associated with "Jan 2017" or "2017-01", or a quarterly measurement might be "2017-Q1".

```{r}
# month
ym("1973-10")
my("Oct 1973")

# quarter
yq("1973-Q1")

```

If we have a variable with annual data, we have to use the parameter `truncated =` to convert our year into a date format:

```{r}
ymd("1973", truncated = 2)
```


By mutating these various representations into a consistent YYYY-MM-DD format, we can now run calculations. In the example below, we calculate the average size of the Canadian labour force in a given year:

```{r}

lfs_canada_employment <- read_rds("data/lfs_canada_employment.rds")

head(lfs_canada_employment)

```

First, we transform the character variable "REF_DATE" variable into a date.

```{r}
lfs_canada_employment <- lfs_canada_employment %>% 
  mutate(REF_DATE = ym(REF_DATE))
lfs_canada_employment
```

With the variable in the <date> type, we can use functions within {lubridate} for our analysis.

To calculate the average employment in each year:

```{r}
lfs_canada_employment %>% 
  group_by(year(REF_DATE)) %>% 
  summarise(mean(VALUE))
```

For the second analysis, we might be interested in determining if there is any seasonality in the Canadian labour force:

```{r}
lfs_canada_employment %>% 
  group_by(month(REF_DATE)) %>% 
  summarise(mean(VALUE))
```



## Cleaning strings

"Strings are not glamorous, high-profile components of R, but they do play a big role in many data cleaning and preparation tasks." —{stringr} reference page

R provides some robust tools for working with characters strings. In particular, the {stringr}[@R-stringr] package is very flexible. But before we get there, we need to take a look at regular expressions.


### Regular expressions

"Some people, when confronted with a problem, think: 'I know, I’ll use regular expressions.' Now they have two problems." — Jamie Zawinski

An excellent resource for learning the basics of regular expressions\index{regular expressions} is the vignette "Regular Expressions", hosted at the {stringr} package website^[["Regular Expressions", stringr.tidyverse.org](https://stringr.tidyverse.org/articles/regular-expressions.html)].


Here are some useful regex matching functions. Note that the square brackets enclose the regex to identify a single character. In the examples below, with three sets of square brackets, "[a][b][c]" contains three separate character positions, whereas "[abc]" is identifying a single character.


| character | what it does |
| ---- | ---- |
| "abc" | matches "abc" |
| "[a][b][c]" | matches "abc" |
| "[abc]" | matches "a", "b", or "c" |
| "[a-c]" | matches any of "a" to "c" <br>(that is, matches "a", "b", or "c") |
| "[^abc]" | matches anything _except_ "a", "b", or "c" |
| "^" | match start of string |
| "$" | match end of string |
| "." | matches any string |



frequency of match

| character | what it does |
| ---- | ---- |
| "?" | 0 or 1 |
| "+" | 1 or more |
| "*" | 0 or more |
| "{n}" | exactly n times |
| "{n,}" | n or more |
| "{n,m}" | between n and m times |


**other special characters**

These also need to be escaped:

| character | what it is |
| ---- | ---- |
| "\"" | double quotation mark |
| "\'" | single quotation mark |
| "\\" | backslash |
| "\\d" | any digit |
| "\\n" | newline (line break) |
| "\\s" | any whitespace (space, tab, newline)
| "\\t" | tab |
| "\\u..." | unicode characters* |

An example of a unicode character^[The full list of unicode characters can be found on Wikipedia[@wiki:unicode].] is the "interobang", a combination of question mark and exclamation mark.

```{r}
interrobang <- "\u2048"
interrobang

```



Canadian postal codes follow a consistent pattern:

* letter digit letter, followed by a space or a hyphen, then digit letter digit


The regex for this is shown below, with the component parts as follows:

* ^ : start of string

* [A-Za-z] : all the letters, both upper and lower case

* \\d : any numerical digit

* [ -]? : space or hyphen; "?" to make either optional (i.e. it may be there or not)

* $ : end of string


```{r}
# regex for Canadian postal codes
# ^ : start of string
# [A-Za-z] : all the letters upper and lower case
# \\d : any numerical digit
# [ -]? : space or hyphen, ? to make either optional (i.e. it may be there or not)
# $ : end of string

postalcode <- "^[A-Za-z]\\d[A-Za-z][ -]?\\d[A-Za-z]\\d$"

```


> "If your regular expression gets overly complicated, try breaking it up into smaller pieces, giving each piece a name, and then combining the pieces with logical operations." —Wickham and Grolemund, _R for Data Science_ [@Wickham_Grolemund2016, p.209]



The regular expression for Canadian postal codes can be shortened, using the case sensitivity operator,  "(?i)". Putting this at the beginning of the expression applies it to all of the letter identifiers in the regex; it can be turned off with "(?-i)".

```{r}

# option: (?i) : make it case insensitive
#         (?-i) : turn off insensitivity
postalcode <- "(?i)^[A-Z]\\d[A-Z][ -]?\\d[A-Z]\\d$(?-i)"

```

The postal code for Kimberley, British Columbia is "V1A 2B3". Here's a list with some variations of that code, which we will use to test our regular expression.

```{r postalcode_kimberley}

# list of postal codes
pc_kimberley <- c("V1A2B3", "V1A 2B3", "v1a 2B3xyz", "v1a-2b3")

```

### Data cleaning with {stringr} 


Armed with the power of regular expressions, we can use the functions within the {stringr} package to filter, select, and clean our data.


```{r load_stringr, eval=FALSE}
library(stringr)
```

As with many R packages, the reference page provides good explanations of the functions in the package, as well as vignettes that give working examples of those functions. The reference page for {stringr} is [stringr.tidyverse.org](https://stringr.tidyverse.org/).



Let's look at some of the functions within {stringr} that are particularly suited to cleaning tasks.



#### string characteristics

| function | purpose |
| ---- | ---- |
| `str_subset()` | character; returns the cases which contain a match |
| `str_which()` | numeric; returns the vector location of matches |
| `str_detect()` | logical; returns TRUE if pattern found |



The {stringr} functions `str_subset()` and `str_which()` return the strings that have the pattern, either in whole or by location.

```{r}
str_subset(pc_kimberley, postalcode)  # which strings have the pattern?
str_which(pc_kimberley, postalcode)   # which strings have the pattern ("e"), by location?

```

Another of the {stringr} functions is `str_detect()`, tests for the presence of a pattern, and returns a logical value (true or false). Here, we test the list of four postal codes using the `postalcode` object that contains the regular expression for Canadian postal codes.

```{r str_detect_postalcode}

str_detect(string = pc_kimberley, pattern = postalcode)

```

As we see, the last example postal code—which contains an extra digit at the end—returns a "FALSE" value. It might contain something that looks like a postal code in the first 7 spaces, but because it has the extra letter, it is not a postal code. To find a string that matches the postal code in any part of a string, we have to change our regex to remove the start "^" and end "$" specifications:


```{r}
# remove start and end specifications
postalcode <- "(?i)[A-Z]\\d[A-Z][ -]?\\d[A-Z]\\d(?-i)"


str_detect(string = pc_kimberley, pattern = postalcode)

```

All of the strings have something that matches the structure of a Canadian postal code. How can we extract the parts that match?

If we rerun the `str_subset()` and `str_which()` functions, we see that all four of the strings are now returned.


```{r}
str_subset(pc_kimberley, postalcode)  # which strings have the pattern?
str_which(pc_kimberley, postalcode)   # which strings have the pattern ("e"), by location?

```

With those two functions, we can identify which strings have the characters that we're looking for. To retrieve a particular string, we can use `str_extract()`:

```{r}

pc_kimberley_all <- str_extract(pc_kimberley, postalcode)
pc_kimberley_all
```



#### replacing and spliting functions


How can we clean up this list so that our postal codes are in a consistent format?

Our goal will be a string of six characters, with upper-case letters and neither a space nor a hyphen. 

This table shows the functions we will use.

| function | purpose |
| ---- | ---- |
| `str_match()` | extracts the text of the match and parts of the match within parenthesis |
| `str_replace()` and `str_replace_all()` | replaces the matches with new text |
| `str_split()` | splits up a string at the pattern |


The first step will be to remove the space or hyphen.

The code below shows two separate lines to achieve this, one for replacing a space, and one for a hyphen, that uses the `gsub()` function in {base} R.  


```{r}

gsub(pattern = " ", replacement = "", x = pc_kimberley_all)
gsub(pattern = "-", replacement = "", x = pc_kimberley_all)

```

Those two steps can be combined by using a single regular expression which replaces either a space or a hyphen:

```{r}

pc_kimberley_clean <- gsub("[ -]", "", pc_kimberley_all)
pc_kimberley_clean


```

There are functions within {stringr} that have the same result, `stringr::str_replace()` and `stringr::str_remove()`.


```{r}

str_replace(string = pc_kimberley_all, pattern = "[ -]", replacement = "")

str_remove(string = pc_kimberley_all, pattern = "[ -]")

```


The second step will be to capitalize the letters. The {stringr} function for this is `str_to_upper`. (There is also parallel `str_to_lower`, `str_to_title`, and `str_to_sentence` functions.)

```{r}
pc_kimberley_clean <- str_to_upper(pc_kimberley_clean)
pc_kimberley_clean
```


If this was in a data frame, these functions can be added to a `mutate()` function, and applied in a pipe sequence, as shown below.

```{r}

# convert string to tibble
pc_kimberley_tbl <- as_tibble(pc_kimberley)

# mutate new values--note that creating three separate variables allows for comparisons as the values change from one step to the next
pc_kimberley_tbl %>%
  mutate(pc_extract = str_extract(value, postalcode)) %>%  # extract the postal codes
  mutate(pc_remove = str_remove(pc_extract, "[ -]")) %>%   # remove space or hyphen
  mutate(pc_upper = str_to_upper(pc_remove))               # change to upper case
  

```



### split and combine strings

Below, we split an address based on comma and space location, using the `str_split()` function.

```{r}
UVic_address <- "Continuing Studies, Continuing Studies Building, 3800 Finnerty Rd, Victoria, BC, V8P 5C2"
  
str_split(UVic_address, ", ")
```



{stringr} contains three useful functions for combining strings. `str_c()` collapses multipe strings into a single string, separated by whatever we specify (the default is nothing, that is to say, `""`.)

Below we will combine the components of an address into a single string, with the components separated by a comma and a space.

```{r}

UVic_address_components <- c("Continuing Studies", "Continuing Studies Building", "3800 Finnerty Rd", "Victoria", "BC", "V8P 5C2")

str_c(UVic_address_components, collapse = ", ")

```

The `str_flatten_comma()` is a shortcut to the same result:

```{r}
str_flatten_comma(UVic_address_components)

```

Note that `str_flatten` could also be used, but we need to explicitly specify the separator:

```{r}

str_flatten(UVic_address_components, collapse = ", ")

```

These functions also have the argument `last =`, which allows us to specify the final separator:


```{r}

str_flatten(UVic_address_components, collapse = ", ", last = " ")

```


### Example: Bureau of Labor Statistics by NAICS code

In this example, we will use regular expressions to filter a table of employment data published by the US Bureau of Labor Statistics. The table shows the annual employment by industry, for the years 2010 to 2020.

With this data, it's important to understand the structure of the file, which is rooted in the North American Industry Classification System (NAICS). This typology is used in Canada, the United States, and Mexico, grouping companies by their primary output. More information can be found in [@StatCan_NAICS].

The NAICS system, and the file we are working with, is hierarchical; higher-level categories subdivide into subsets. Let's examine employment in Mining, Quarrying, and Oil and Gas Extraction sector. 

This sector's coding is as follows:

| Industry | NAICS code |
| ---- | ---- |
| Mining, Quarrying, and Oil and Gas Extraction | 21 |
| * Oil and Gas Extraction | 211 |
| * Mining (except Oil and Gas) | 212 |
|   - Coal Mining | 2121 |
|   - Metal Ore Mining | 2122 |
|   - Nonmetallic Mineral Mining and Quarrying | 2123 |
| Support Activities for Mining | 213 |


Let's look at the data now:


```{r}

bls_employment <- read_csv("data/us_bls_employment_2010-2020.csv")
head(bls_employment)

```

There are three "supersectors" in the NAICS system: primary industries, goods-producing industries, and service-producing industries. The "Series ID" variable string starts with "CEU", then the digits in positions 4 and 5 represent the super-sector. The first row, supersector "00", is the aggregation of the entire workforce.

Mining, Quarrying, and Oil and Gas Extraction is a sector within the goods-producing industries, with the NAICS code number 21. This is represented at digits 6 and 7 of the string. 

Mining (NAICS 212) is a subsector, comprised of three industries (at the four-digit level).

The hierarchical structure of the dataframe means that if we simply summed the column we would end up double-counting: the sum of coal, metal ore, and non-metalic mineral industry rows is reported as the employment in Mining (except Oil and Gas).


Let's imagine our assignment is to produce a chart showing the employment in the three subsectors, represented at the three-digit level.

First, let's look the rows associated with the sector, NAICS 21. 

```{r}

# NAICS 212
bls_employment %>% 
  filter(str_detect(`Series ID`, "CEU1021"))

```

In the table above, we see the first row has the "Series ID" of CEU1021000001. That long string of zeros shows that this is the sector, NAICS 21. The employment total for each year is the aggregation of the subsectors 211, 212, and 213. As well, this table also shows the industries within 212, starting with  CEU1021210001 (NAICS 2121).

What if we selected only the values that have a zero in the ninth spot?

```{r}

bls_employment %>% 
  # start with previous table
  filter(str_detect(`Series ID`, "CEU1021")) %>% 
  # remove the ones that have a zero in the 5th spot from the end
  filter(str_detect(`Series ID`, "00001$"))

```

That's not quite right—it includes the "21" sector aggregate.

Let's filter out the rows that have a zero after the "21" using an exclamation mark at the beginning of our `str_detect()` function.

```{r}
# NAICS 212
bls_employment %>% 
  # start with previous table
  filter(str_detect(`Series ID`, "CEU1021")) %>% 
  # use the exclamation mark to filter those that don't match
  filter(!str_detect(`Series ID`, "CEU10210"))

```

That doesn't work, because it leaves in the four-digit industries, starting with 2121.

The solution can be found by focussing on the characters that  Let's add the digits 1 to 9 to the front of the second filter:

```{r}

# NAICS 212
bls_employment %>% 
  # start with previous table
  filter(str_detect(`Series ID`, "CEU1021")) %>% 
  # remove the ones that have a zero in the 5th spot from the end
  # - add any digit from 1-9
  filter(str_detect(`Series ID`, "[1-9]00001$"))

```

A second option would be to use a negate zero, "[^0]". You'll notice that this uses the carat symbol, "^". Because it is inside the square brackets, it has the effect of negating the named values. This is different behaviour than when it's outside the square brackets and means "start of the string".

```{r}

# NAICS 212
bls_employment %>% 
  # start with previous table
  filter(str_detect(`Series ID`, "CEU1021")) %>% 
  # remove the ones that have a zero in the 6th spot from the end
  # - add any digit that is not a zero
  filter(str_detect(`Series ID`, "[^0]00001$"))

```


One final option, using `str_sub()` to specify the location, inside the `str_detect()` function.


```{r}

# NAICS 212
bls_employment %>% 
  # start with previous table
  filter(str_detect(`Series ID`, "CEU1021")) %>% 
  # remove the ones that have a zero in the 6th spot from the end
  # - add any digit that is not a zero

  # work
#    filter(str_sub(`Series ID`, start = 1, end = 2) == "CE")
#  filter(str_sub(`Series ID`, start = 8, end = 9) == "00")

  # doesn't work
  #  filter(str_sub(`Series ID`, start = 8, end = 9) == "[0][0]")
  filter(str_detect(str_sub(`Series ID`, start = 8, end = 9), "[^0][0]"))

#str_sub(bls_employment$`Series ID`, start = 8, end = 9)
  
```

#### NOC: split code from title

This example is drawn from a real-life case, where the NOC (National Occupation Classification) code is in the same column as the code's title.

It's occupations in B.C.'s accommodation industry^[Copied from go2HR report ["Accommodation Workforce Profile"](https://www.go2hr.ca/research/bc-tourism-and-hospitality-labour-market-information-workforce-profile#1657831995289-efea257d-563d)]


```{r read_noc, include=FALSE}

noc_table <- read_rds(here("data", "noc_table.rds"))

noc_table

```

This table has four variables in three columns, each of which has at least one data cleaning challenge.

**`noc_occupation_title`**

This variable combines two values: the three-digit NOC code, and the title of the occupation.

We will use the {tidyr} function `separate()` to split it into two variables. 

* https://tidyr.tidyverse.org/reference/separate.html

```{r}

noc_table_clean <- noc_table %>% 
  tidyr::separate(
    col = noc_occupation_title, 
    into = c("noc", "occupation_title"), 
    sep = ": ")

noc_table_clean

```

(See more complex version below)

**`certification_training_requirements`**

This variable has a weird square character at the beginning, and also a space between the weird character and the first letter. 

Here we can use the {stringr} function `str_remove_all()`

```{r}

noc_table_clean <- noc_table_clean %>% 
  mutate(
    certification_training_requirements =
      str_remove_all(certification_training_requirements, " ")
    )

noc_table_clean

```


**`employment`**

This variable should be numbers, but is a character type. This is because

* there are commas used as the thousands separator

* the smallest categories are represented with "-*" to indicate a table note.

This requires three steps:

1. remove commas

2. replace "-*" with NA

3. convert to numeric value

```{r}

noc_table_clean <- noc_table_clean %>% 
  # remove commas
  mutate(
    employment =
      str_remove_all(employment, ",")
    ) %>% 
  # replace with NA
  mutate(
    employment =
      str_replace_all(employment, "-\\*", replacement = NA_character_)
    ) %>% 
  # convert to numeric
  mutate(
    employment =
      as.numeric(employment)
    )
  

noc_table_clean

```



In the first column, there was a colon and a separating the NOC code and the title. What would we do if there was only a space? 

The revised table explores that.

```{r read_noc_table_2, include=FALSE}
noc_table_2 <- read_rds(here("data", "noc_table_2.rds"))
```


There are multiple spaces in the string...what happens to the split if we change the code to `sep = " "`?

```{r}
noc_table_2 %>% 
  tidyr::separate(
    col = noc_occupation_title, 
    into = c("noc", "occupation_title"), 
    sep = " ")
```

The function keeps only the first two pieces...so a lot of valuable information that follows the second string (the first word of the title) is discarded.





## Creating conditional and calculated variables

Recoding with `dplyr::case_when()`


## Indicator variables

Creating indicator variables \index{indicator variables}

> To include a categorical variable in a regression, a natural approach is to construct an 'indicator variable' for each category. This allows a separate effect for each level of the category, without assuming any ordering or other structure on the categories. [@Gelman_etal_2014, p.366]

* also known as dummy variables\index{dummy variables|see{indicator variables}} [@Suits_1957] and "one hot" encoding\index{one hot encoding|see{indicator variables}}

Examples of this approach abound. It can be a useful approach in forecasting methods; Hyndman and Athanasopoulos provide examples where public holidays and days of the week are set as dummy variables [@Hyndman_Athanasopoulos_2021, section 7.4 "Some useful predictors"]

It is also a common approach in social data analysis, where categorical variables are used to code information. In the section on "Discrimination and collider bias" in [@Cunningham_mixtape, pp.106–110], the data are represented in the following way ^[Note: the data in this table is different from what appears in _Causal Inference: The Mixtape_, since the values in the data in that source are randomly generated.]:

```{r ch700_mixtape_data, echo=FALSE}
set.seed(1066) # The rural framework was complete


# code directly from _Causal Mixtape_ https://mixtape.scunning.com/dag.html?panelset=r-code
# except it generates 10 cases, not 10000
tb <- tibble(
  female = ifelse(runif(10)>=0.5,1,0),
  ability = rnorm(10),
  discrimination = female,
  occupation = 1 + 2*ability + 0*female - 2*discrimination + rnorm(10),
  wage = 1 - 1*discrimination + 1*occupation + 2*ability + rnorm(10) 
)

head(tb)
```

In this data, the variable `female` is coded as numeric, so it can be used as part of the regression modeling. 

More commonly, though, this variable will have been captured and saved in one called "gender" or "sex", and often as a character string. The source data for the above table might have originally looked like this:

```{r}

tb2 <- tb %>% 
  dplyr::rename(gender = female) %>% 
  mutate(gender = case_when(
    gender == 1 ~ "female",
    TRUE ~ "male"
  ))


tb2
```



### `{fastDummies}` 

Because this type of data transformation is common, the package {fastDummies} [@R-fastDummies] has been created, containing the function `dummy_cols()` (or `dummy_columns()` if you prefer extra typing) that creates indicator variables. 

> If you are using the _tidymodels_ pipeline, the {recipes} package [@R-recipes] contains the `step_dummy()` function, which accomplishes much the same result as `fastDummies::dummy_cols()`. The book _Tidy Modeling with R_ by Max Kuhn and Julia Silge [@Kuhn_Silge_2022] is recommended; the creation of indicator variables is covered in Chapter 8 "Feature Engineering with recipes".



```{r}

tb2 %>% 
  fastDummies::dummy_cols() 

```

The variable `gender` has now been mutated into two additional variables, `gender_female` and `gender_male`. The names of the new variables are a concatenation of the original variable name and value, separated by an underscore.

Where the value of `gender` is "female", the value of `gender_female` is assigned as 1, and where `gender` is "male", `gender_female` is 0. The opposite is true of `gender_male`; the rows where the value of `gender` is "male" `gender_male` has the value 1.


Multicollinearity can occur in a multiple regression model where multiple indicator variables are included, since they are the inverse of one another. Accordingly, an important set of options in the `dummy_cols()` function revolve around removing all but one of the created variables. The argument `remove_first_dummy = TRUE` does just that; in this case, the `gender_female()` variable does not appear in the final result. This is because the new indicator variables are created in alphabetic order ("female" coming before "male"), and the first is dropped.

{Note to self: can this be controlled? To match the original dataframe we might want the `gender_female` variable to be the one that persists...}

```{r}

tb2 %>% 
  fastDummies::dummy_cols(remove_first_dummy = TRUE) 

```

Note that the results of this argument can be controlled through the conversion of character variables to factors.

```{r}
# create new tb table with "gender" as factor
tb3 <- tb2 %>% 
  mutate(gender = as.factor(gender)) 

levels(tb3$gender)
```

Note that the default order is alphabetical, so "female" remains first. When the argument `remove_first_dummy = TRUE` is applied, we get the same result as before.

```{r}
  
tb3 %>% 
  fastDummies::dummy_cols(remove_first_dummy = TRUE) 


```

By using the `fct_relevel()` function from the {forcats}[@R-forcats] package, we can assign an arbitrary order.

```{r}
# reorder levels 

tb3$gender <- fct_relevel(tb3$gender, "male", "female")

levels(tb3$gender)
```

Rerunning the same code, the `gender_male` variable is dropped.

```{r}
  
tb3 %>% 
  fastDummies::dummy_cols(remove_first_dummy = TRUE) 


```


There is also an option in `fastDummies::dummy_cols()` to `remove_most_frequent_dummy = TRUE`. In our discrimination data, there are more observations coded as "female", so the variable `gender_female` is not included.


```{r}

tb2 %>% 
  fastDummies::dummy_cols(remove_most_frequent_dummy = TRUE) 


```


Another argument in the function is `remove_selected_columns`. The default value is `FALSE`, but if the argument is set to `TRUE`, the source column is excluded from the output. 

```{r}

tb2 %>% 
  fastDummies::dummy_cols(remove_selected_columns = TRUE) 

```



Some important things to note about `dummy_cols()` 

* the function will create as many indicator variables as there are values in the character or factor variable

* the function will create indicator variables for all of the character and factor variables in the data, unless otherwise specified.

Let's look at how `dummy_cols()` behaves with two of the variables in {palmerpenguins}: \index{palmerpenguins}  

```{r ch700_dummy_penguins_1, echo=FALSE}

library(palmerpenguins)

set.seed(1729) #Ramanujan's taxicab number: 1^3 + 12^3 = 9^3 + 10^3

penguin_subset <- penguins %>% 
  slice_sample(n = 6) %>% 
  dplyr::select(species, island) %>% 
  arrange(species)

penguin_subset

```

```{r ch700_dummy_penguins_2}

penguin_subset %>%  
  fastDummies::dummy_cols()

```

First, you will notice that we started with two factor variables, `species` and `island`, we now have indicator variables for both.

And because there are three species of penguins and three islands, we have three indicator variables for both of the original variables.

In the instance where we have multiple character or factor variables in our source data, we can control which variables get the `dummy_cols()` treatment with the option `select_columns`. In the example below, the result provides us with an indicator variable for `species` but not `island`.

In addition, rather than dropping the first created variable, we can specify that the most frequently occurring value be removed with the argument `remove_most_frequent_dummy = TRUE`. Because there are more Gentoo penguins in our sample of the data, the indicator variable `species_Gentoo` is not created.

```{r 700_penguin_dummy}


penguin_subset %>% 
  fastDummies::dummy_cols(
    select_columns = "species",
    remove_most_frequent_dummy = TRUE)


```


Later: add dummy rows with `dummy_rows()`

### a tidyr::pivot_longer() solution

https://twitter.com/_TanHo/status/1415100126272577536?s=20&t=VsY7hHNn01lS17_eXvUvhA  

https://gist.github.com/tanho63/50d9b323e29165ad3e027bc3cf1c5926

https://tidyr.tidyverse.org/articles/pivot.html#multiple-observations-per-row


## Missing values

\index{missing values}



Frequently, our data will have missing values. There are a variety of solutions to dealing with missingness, from listwise deletion (removing all rows with missing values) to complex algorithms that apply methods to impute the best estimate of the missing value.

### Strategies for dealing with missing values

Explicit user-defined missing: convert -999 etc to `NA`

Imputation methods:

* fill

* average or regression model estimate

* hot deck imputation


### Missing as formatting


A common spreadsheet formatting practice that makes the tables human-readable but less analysis-ready is to have a sub-section heading, often in a separate column. In our example Excel file, the sheet "Report" contains the data we want in a wide format. The name individual institution is in the second column (in Excel nomenclature, "B"), and the region ("Economic Development Region") is in the first column. However, the name of the region only appears in the row above the institutions in that region. This makes for a nicely formatted table, but isn't very helpful if we are trying to calculate average tuition fees by region.^[The creators of the file have provided us with a tidy sheet named "Data", and deserve kudos for sharing the data in a variety of layouts. But please be aware that this is approach the exception, not the rule.]

Our first step is to read in the data. Note that the code uses the Excel naming convention to define the rectangular range to be read, and omits the title rows at the top and the notes at the bottom of the sheet.

```{r}

tuition_data <- readxl::read_excel(dpjr::dpjr_data("intl_tuition_fees_bc.xlsx"),
                                   sheet = "Report",
                                   range = "A5:L36")

head(tuition_data)

```

The contents of the file have been read correctly, but there are many "NA" values in the "Economic Development Region" variable. The `fill()` function, in the {tidyr} package [@R-tidyr] [\index{{tidyr}}], provides the solution to this. The function replaces a missing value with either the previous or the next value in a variable. In the example here, it will replace all of the "NA" values with the region name above, stopping when it comes to the next non-NA record.

```{r}

tuition_data_filled <- tuition_data %>% 
  fill("Economic Development Region")

head(tuition_data_filled)
```

The next step is to use the {tidyr} function `drop_na()` to omit any row that contains an NA value. In this case, we want to drop what were the header rows that contained just the region name in the first column, and no values in the other columns.

These two solutions yield the same result:

```{r}
drop_na(tuition_data_filled)

tuition_data_filled %>% 
  drop_na()

```

It's worth noting that the `drop_na()` function provides an argument where we can specify columns. This gives us the flexibility to drop some rows where there is an NA in a specific variable, while keeping other rows that have a value in that variable but not in other variables.


## Missing numeric values


"Missing data are unobserved values that would be meaningful for analysis if observed; in other words, a missing value hides a meaningful value." [@Little_Rubin_2020 ]

The functions used above for filling missing text values can also be used for missing numeric values. But when dealing with the replacement of numeric values we need to be mindful that the values we use will be incorporated into models and analysis; we need to be clear that the filled values are _estimates_, including sufficient documentation in our methodology section.

Furthermore, there is a great deal of literature on the topic of imputation of missing values. As researchers, we need to be aware of _why_ the values are missing; are we dealing with values that are missing "at completely random", "at random", or "not at random"? Delving into the literature, you'll discover that these terms are precisely defined, and the solutions we adopt to deal with missingness will depend on why the data are missing. [@Little_Rubin_2020]

For the examples presented here, we will assume that you have already given significant thought to these methodological questions. As we discussed earlier in this chapter, _how_ we clean data is influenced by our research questions. As well, we need to consider our cleaning strategy in light of why the raw data are the way they are.

With that preamble, let's move to working with some real data. In this example, the data series is the number of salaried employees in the Accommodation services industry in British Columbia, Canada, for the years 2001 through 2020. The data is collected and published by Statistics Canada, and for some years the data are judged to be "too unreliable to be published". This leaves some gaps in our data. [@R-dpjr]



```{r read_naics721, echo=FALSE}

naics721_sal <- read_rds(dpjr::dpjr_data("seph_naics721_bc_sal.rds"))

head(naics721_sal)

```

Plotting the data, here's what we see:

```{r}
ggplot(naics721_sal, aes(x = ref_year, y = VALUE)) +
  geom_line() 
```


Our first approach will be to simply fill the series with the previous valid entry.  

Similar to what we saw above with text, the same `fill()` function can be applied to numeric variables.





{tidyr} functions to deal with the missing values in the series


* drop the rows that have an NA value

```{r}
  
naics721_sal %>% 
  tidyr::drop_na()

```


* `tidyr::fill()` variants

```{r}
# fill down  
naics721_sal %>% 
  mutate(old_value = VALUE) %>% 
  tidyr::fill(VALUE)  # default direction is down

# specified
# down
naics721_sal %>% 
  mutate(down = VALUE) %>% 
  tidyr::fill(VALUE, .direction = "down")  # direction is specified as down
# up
naics721_sal %>% 
  mutate(up = VALUE) %>% 
  tidyr::fill(VALUE, .direction = "up")  # direction is specified as up


```





### Time series imputation

You may already have realized that the values we filled above are drawn from a time series. Economists, biologists, and many other researchers who work with time series data have built up a large arsenal of tools to work with time series data, including for dealing with missing values in a time series. 

For R users, the package {forecast} has time series oriented imputation fill functions. [@R-forecast; @Hyndman_Khandakar_2008]


The `na.interp()` function from {forecast} gives us a much better way to fill the missing values in our time series. This function, by default, uses as linear interpolation to calculate values between two non-missing values. Here's a simple example

```{r}
x <- c(1, NA, 3)
x
```

The series from 1 to 3 has an NA value in the middle. The `na.interp()` function fills in the missing value.

```{r}

forecast::na.interp(x)
```

For longer series, a similar process occurs:

```{r}
y <- c(1, NA, NA, 3)
y
```

```{r}
forecast::na.interp(y)

```

You will notice that the function has calculated replacement values that give us a linear series.

For our count of salaried employees, here's what we get when we create a new value `value_new` with this function:

```{r}
naics721_sal %>% 
  mutate(value_new = forecast::na.interp(VALUE))
```

The function also has the capacity to create non-linear models.



Other examples can be found in [@Hyndman_Athanasopoulos_2021].




### {imputeTS} package

{imputeTS} [@R-imputeTS] offers multiple "imputation algorithm implementations along with plotting functions for time series missing data statistics."



The {imputeTS} package has functions that implement more sophisticated time series imputation methods, such as those with seasonal components.

https://CRAN.R-project.org/package=imputeTS




### {zoo} fill functions

Somewhat similar to {forecast}, the {zoo} package has a range of functions for time series data.

CRAN page: https://cran.r-project.org/web/packages/zoo/index.html



```{r zoo_na_approx}
library(zoo)

naics721_sal %>%
  mutate(old_value = VALUE) %>% 
  mutate(VALUE = zoo::na.approx(VALUE))

```



### references

* CRAN page: https://cran.r-project.org/web/packages/zoo/index.html

* https://www.elff.eu/book/data-management-r/07-dates-times-timeseries/handling-missing-values/


{simputation}: https://cran.r-project.org/web/packages/simputation/vignettes/intro.html

https://github.com/markvanderloo/simputation


[@doi:10.1080/00223891.2018.1530680]


{forecast} examples:

* https://www.geo.fu-berlin.de/en/v/soga/Geodata-analysis/time-series-analysis/Dealing-with-missing-values/Imputing-missing-values/index.html




## Creating labelled factors from numeric variables

If you are familiar with statistical software packages such as SPSS, SAS, or Stata you will be used to working with labelled variables. These are categorical variables, where there are a limited number of categories. 

In some cases, there might be one or more values assigned to categories we might want to designate as "missing"—a common example can be found in surveys, where respondents are given the option of answering "don't know" and "not applicable". While these responses might be interesting in and of themselves, in some contexts we will want to assign them as missing.

In other cases, numeric values are used to store the categorical responses. 

The Joint Canada/United States Survey of Health (JCUSH)\index{Joint Canada/United States Survey of Health (JCUSH)} file we saw in the section on importing fixed width files gives us a good example of where categories are stored as numerical variables. For example, the variable "SPJ1_TYP" is either a "1" for responses from the Canadian sample, and a "2" for the sample from the United States. 

We will use this file to create labelled factors from the numeric values in the variables we imported. Here's the code we used previously to read the file:

```{r jcush_txt_read_vars_2}

jcush <- readr::read_fwf(dpjr::dpjr_data("JCUSH.txt"), 
         fwf_cols(
           SAMPLEID = c(1, 12),
           SPJ1_TYP = c(13, 13), 
           GHJ1DHDI = c(32, 32),
           SDJ1GHED = c(502, 502)
           ),
         col_types = list(
           SAMPLEID = col_character()
         ))

head(jcush)
```


The values for these variables are as follows:


| Name | Variable | Code | Value |
| :--- | :---     | :---:  | :---:  |
| SAMPLEID | Household identifier | unique number | &nbsp; |
| SPJ1_TYP | Sample type [country] | 1 | Canada |
| &nbsp; | &nbsp; | 2 | United States |
| GHJ1DHDI | Health Description Index | 0 | Poor |
| &nbsp; | &nbsp; | 1 | Fair |
| &nbsp; | &nbsp; | 2 | Good |
| &nbsp; | &nbsp; | 3 | Very Good |
| &nbsp; | &nbsp; | 4 | Excellent |
| &nbsp; | &nbsp; | 9 | Not Stated |
| SDJ1GHED | Highest level of post-secondary education attained | 1 | Less than high school |
| &nbsp; | &nbsp; | 2 | High school degree or equivalent (GED) |
| &nbsp; | &nbsp; | 3 | Trades Cert, Voc. Sch./Comm.Col./CEGEP |
| &nbsp; | &nbsp; | 4 | Univ or Coll. Cert. incl. below Bach. |
| &nbsp; | &nbsp; | 9 | Not Stated |



The variable "GHJ1DHDI" has the respondent's self-assessment of their overall health. Here's a summary table of the responses:

```{r}
jcush %>% 
  group_by(GHJ1DHDI) %>% 
  tally()

```

But it would be much more helpful and efficient if we attach the value labels to the dataframe.

One strategy is to create a new factor variable, that uses the value description instead of the numeric representation. For this, we will use the `fct_recode()` function from the {forcats} package[@R-forcats].

```{r}
library(forcats)

jcush_forcats <- jcush %>%
  # 1st mutate a new factor variable with the original values
  mutate(health_desc_fct = as.factor(GHJ1DHDI)) %>%
  # 2nd recode the values
  mutate(
    health_desc_fct = fct_recode(
      health_desc_fct,
      "Poor" = "0",
      "Fair" = "1",
      "Good" = "2",
      "Very Good" = "3",
      "Excellent" = "4",
      NULL = "9"
    )
  )


```

The `levels()` function allows us to inspect the labels. Note that because we specified the original value of "9" as `NULL`, it does not appear in the list.

```{r}
levels(jcush_forcats$health_desc_fct)
```

When we tally the results by our new variable, the "GHJ1DHDI" records with the value 9 are shown as "NA".

```{r}
jcush_forcats %>% 
  group_by(GHJ1DHDI, health_desc_fct) %>% 
  tally()

```


{haven}[@R-haven] provides another option.

```{r}
library(haven)

jcush_haven <- jcush %>%
  mutate(health_desc_lab = labelled(
    GHJ1DHDI,
    c(
      "Poor" = 0,
      "Fair" = 1,
      "Good" = 2,
      "Very Good" = 3,
      "Excellent" = 4,
      "Not Stated" = 9
    )
  ))

```

Note that our new variable is a "S3: haven_labelled" class, and the `ls.str()` function shows it as "dbl+lbl":

```{r}
ls.str(jcush_haven)
```


The `levels()` function can be used to inspect the result of this mutate.

```{r}

head(jcush_haven$health_desc_lab)

```

```{r}

# note: `as_factor()` works; `as.factor()` does not
levels(as_factor(jcush_haven$health_desc_lab))             
                                   
levels(as.factor(jcush_haven$health_desc_lab))             

```


A third option is to use the functions within the {labelled}[@R-labelled] package. 

```{r}
jcush_labelled <- jcush %>%
  mutate(health_desc_lab = labelled(
    GHJ1DHDI,
    c(
      "Poor" = 0,
      "Fair" = 1,
      "Good" = 2,
      "Very Good" = 3,
      "Excellent" = 4,
      "Not Stated" = 9
    )
  ))

jcush_labelled
```


```{r}

head(jcush_labelled$health_desc_lab)

```

```{r}

# note: `as_factor()` works; `as.factor()` does not
levels(as_factor(jcush_labelled$health_desc_lab))             
                                   
levels(as.factor(jcush_labelled$health_desc_lab))             

```

This package also allows for a descriptive name of the variable to be appended with the `var_lab()` function.


### Base R: `model.matrix`

[@Friendly_Meyer_2016, p.240]








