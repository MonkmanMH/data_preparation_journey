---
output:
  pdf_document: default
  html_document: default
#  word_document: default
---


<!-- 
This file by Martin Monkman is licensed under a Creative Commons Attribution 4.0 International License. 
-->



```{r setup_ch700, include=FALSE}

# packages

source("package_load.R")

```


# Cleaning techniques {#cleaning}

In this chapter:


* Cleaning dates and strings

* Creating conditional and calculated variables

  - indicator variables (also known as dummy variables [@Suits_1957] and "one hot" encoding)
  
* Dealing with missing values


## Introduction

We have already applied some cleaning methods. In the chapters on importing data, we tackled a variety of techniques that would fall into "data cleaning", including assigning variable types and restructuring data into a tidy format. In this chapter, we look in more depth at some examples of recurring data cleaning challenges.


## Cleaning dates

A recurring challenge in many data analysis projects is to deal with date and time fields. One of the first steps of any data cleaning project should be to ensure that these are in a consistent format that can be easily manipulated.

As we saw in [@Broman_Woo_2017], there is an international standard (ISO 8601)\index{ISO 8601} for an unambiguous and principled way to write dates and times (and in combination, date-times). For dates, we write **YYYY-MM-DD**. Similarly, time is written **T[hh][mm][ss]** or **T[hh]:[mm]:[ss]**, with hours (on a 24-hour clock) followed by minutes and seconds.

When we are working with dates, the objective of our cleaning is to transform the date into the ISO 8601 standard and then, where appropriate, transform that structure into a date type variable. The benefits of the data type format are many, not the least of which are the R tools built to streamline analysis and visualization that use date type variables, [@Wickham_Grolemund2016, Chapter 16] and those same tools allow us to tackle cleaning variables that capture date elements. 

One such package is {lubridate}, part of the tidyverse.

### Creating a date-time field

The first way that dates might be represented in your data is as three separate fields, one for year, one for month, and one for day. These can be assembled into a single date variable using the `make_date()`, or a single datetime variable using `make_datetime()`.

In the {Lahman}[@R-Lahman] table "People", each player's birth date is stored as three separate variables. They can be combined into a single variable using `make_date()`:

```{r}
Lahman::People |> 
  slice_head(n = 10) |> 
  select(playerID, birthYear, birthMonth, birthMonth, birthDay) |>
  # create date-time variable
   mutate(birthDate = make_datetime(birthYear, birthMonth, birthDay)) 

```


The second problematic way dates are stored is in a variety of often-ambiguous sequences. This might included text strings for the month (for example "Jan", "January", or "Janvier" rather than "01"), two-digit years (does "12" refer to 2012 or 1912?), and ambiguous ordering of months and days (does "12" refer to December or the twelfth day of the month?)

If we know the way the data is stored, we can use {lubridate} functions to process the data into YYYY-MM-DD. These three very different date formats can be transformed into a consistent format:

```{r}
ymd("1973-10-22")
mdy("October 22nd, 1973")
dmy("22-Oct-1973")
```


A third manner in which dates can be stored is with a truncated representation. For example, to total sales for the entire month might be recorded as a single value associated with "Jan 2017" or "2017-01", or a quarterly measurement might be "2017-Q1".

```{r}
# month
ym("1973-10")
my("Oct 1973")

# quarter
yq("1973-Q1")

```

If we have a variable with annual data, we have to use the parameter `truncated =` to convert our year into a date format:

```{r}
ymd("1973", truncated = 2)
```


By mutating these various representations into a consistent YYYY-MM-DD format, we can now run calculations. In the example below, we calculate the average size of the Canadian labour force in a given year:

```{r}

lfs_canada_employment <- 
  read_rds(dpjr::dpjr_data("lfs_canada_employment.rds"))

```

First, we transform the character variable "REF_DATE" variable into a date.

```{r}
lfs_canada_employment <- lfs_canada_employment |> 
  mutate(REF_DATE = ym(REF_DATE))

```

With the variable in the date type, we can use functions within {lubridate} for our analysis.

To calculate the average employment in each year:

```{r}
lfs_canada_employment |> 
  group_by(year(REF_DATE)) |> 
  summarise(mean(VALUE))
```

For the second analysis, we might be interested in determining if there is any seasonality in the Canadian labour force:

```{r}
lfs_canada_employment |> 
  group_by(month(REF_DATE)) |> 
  summarise(mean(VALUE))
```



## Cleaning strings

"Strings are not glamorous, high-profile components of R, but they do play a big role in many data cleaning and preparation tasks." —{stringr} reference page

R provides some robust tools for working with characters strings. In particular, the {stringr}[@R-stringr] package is very flexible. But before we get there, we need to take a look at regular expressions. \index{regular expressions}



### Regular expressions

"Some people, when confronted with a problem, think: 'I know, I’ll use regular expressions.' Now they have two problems." — Jamie Zawinski

Regular expressions are a way to specify a pattern of characters. This pattern can be specific letters or numbers ("abc" or "123") or character types (letters, digits, white space, etc.), or a combination of the two.

An excellent resource for learning the basics is the vignette "Regular Expressions", hosted at the {stringr} package website^[["Regular Expressions", stringr.tidyverse.org](https://stringr.tidyverse.org/articles/regular-expressions.html)].


Here are some useful regex matching functions. Note that the square brackets enclose the regex to identify a single character. In the examples below, with three sets of square brackets, "[a][b][c]" contains three separate character positions, whereas "[abc]" is identifying a single character.


| character | what it does |
| ---- | ---- |
| "abc" | matches "abc" |
| "[a][b][c]" | matches "abc" |
| "[abc]" | matches "a", "b", or "c" |
| "[a-c]" | matches any of "a" to "c" <br>(that is, matches "a", "b", or "c") |
| "[^abc]" | matches anything _except_ "a", "b", or "c" |
| "^" | match start of string |
| "$" | match end of string |
| "." | matches any string |





| character | frequency of match |
| ---- | ---- |
| "?" | 0 or 1 |
| "+" | 1 or more |
| "*" | 0 or more |
| "{n}" | exactly n times |
| "{n,}" | n or more |
| "{n,m}" | between n and m times |


These also need to be escaped:

| character | what it is |
| ---- | ---- |
| "\"" | double quotation mark |
| "\'" | single quotation mark |
| "\\" | backslash |
| "\\d" | any digit |
| "\\n" | newline (line break) |
| "\\s" | any whitespace (space, tab, newline)
| "\\t" | tab |
| "\\u..." | unicode characters* |

An example of a unicode character^[The full list of unicode characters can be found on Wikipedia[@wiki:unicode].] is the "interobang", a combination of question mark and exclamation mark.

```{r}
interrobang <- "\u2048"
interrobang

```



Canadian postal codes follow a consistent pattern:

* letter digit letter, followed by a space or a hyphen, then digit letter digit


The regex for this is shown below, with the component parts as follows:

* ^ : start of string

* [A-Za-z] : all the letters, both upper and lower case

* \\d : any numerical digit

* [ -]? : space or hyphen; "?" to make either optional (i.e. it may be there or not)

* $ : end of string


```{r}
# regex for Canadian postal codes
# ^ : start of string
# [A-Za-z] : all the letters upper and lower case
# \\d : any numerical digit
# [ -]? : space or hyphen, 
#         ? to make either optional (i.e. it may be there or not)
# $ : end of string

postalcode <- "^[A-Za-z]\\d[A-Za-z][ -]?\\d[A-Za-z]\\d$"

```


> "If your regular expression gets overly complicated, try breaking it up into smaller pieces, giving each piece a name, and then combining the pieces with logical operations." —Wickham and Grolemund, _R for Data Science_ [@Wickham_Grolemund2016, p.209]



The regular expression for Canadian postal codes can be shortened, using the case sensitivity operator,  "(?i)". Putting this at the beginning of the expression applies it to all of the letter identifiers in the regex; it can be turned off with "(?-i)".

```{r}

# option: (?i) : make it case insensitive
#         (?-i) : turn off insensitivity
postalcode <- "(?i)^[A-Z]\\d[A-Z][ -]?\\d[A-Z]\\d$(?-i)"

```

The postal code for Kimberley, British Columbia is "V1A 2B3". Here's a list with some variations of that code, which we will use to test our regular expression.

```{r postalcode_kimberley}

# list of postal codes
pc_kimberley <- c("V1A2B3", "V1A 2B3", "v1a 2B3xyz", "v1a-2b3")

```

### Data cleaning with {stringr} 


Armed with the power of regular expressions, we can use the functions within the {stringr} package to filter, select, and clean our data.


```{r load_stringr, eval=FALSE}
library(stringr)
```

As with many R packages, the reference page provides good explanations of the functions in the package, as well as vignettes that give working examples of those functions. The reference page for {stringr} is [stringr.tidyverse.org](https://stringr.tidyverse.org/).



Let's look at some of the functions within {stringr} that are particularly suited to cleaning tasks.



#### string characteristics

| function | purpose |
| ---- | ---- |
| `str_subset()` | character; returns the cases which contain a match |
| `str_which()` | numeric; returns the vector location of matches |
| `str_detect()` | logical; returns TRUE if pattern found |



The {stringr} functions `str_subset()` and `str_which()` return the strings that have the pattern, either in whole or by location.

```{r}
# which strings have the pattern?
str_subset(pc_kimberley, postalcode)  

# which strings have the pattern ("e"), by location?
str_which(pc_kimberley, postalcode)   

```

Another of the {stringr} functions is `str_detect()`, tests for the presence of a pattern, and returns a logical value (true or false). Here, we test the list of four postal codes using the `postalcode` object that contains the regular expression for Canadian postal codes.

```{r str_detect_postalcode}

str_detect(string = pc_kimberley, pattern = postalcode)

```

As we see, the third example postal code—which contains an extra digit at the end—returns a "FALSE" value. It might contain something that looks like a postal code in the first 7 spaces, but because it has the extra letter, it is not a postal code. To find a string that matches the postal code in any part of a string, we have to change our regex to remove the start "^" and end "$" specifications:


```{r}
# remove start and end specifications
postalcode <- "(?i)[A-Z]\\d[A-Z][ -]?\\d[A-Z]\\d(?-i)"


str_detect(string = pc_kimberley, pattern = postalcode)

```

All of the strings have something that matches the structure of a Canadian postal code. How can we extract the parts that match?

If we rerun the `str_subset()` and `str_which()` functions, we see that all four of the strings are now returned.


```{r}
# which strings have the pattern?
str_subset(pc_kimberley, postalcode)

# which strings have the pattern ("e"), by location?
str_which(pc_kimberley, postalcode)   

```

With those two functions, we can identify which strings have the characters that we're looking for. To retrieve a particular string, we can use `str_extract()`:

```{r}

pc_kimberley_all <- str_extract(pc_kimberley, postalcode)
pc_kimberley_all
```



#### replacing and spliting functions


How can we clean up this list so that our postal codes are in a consistent format?

Our goal will be a string of six characters, with upper-case letters and neither a space nor a hyphen. 

This table shows the functions we will use.

| function | purpose |
| ---- | ---- |
| `str_match()` | extracts the text of the match and parts of the match within parenthesis |
| `str_replace()` and `str_replace_all()` | replaces the matches with new text |
| `str_split()` | splits up a string at the pattern |


The first step will be to remove the space or hyphen.

The code below shows two separate lines to achieve this, one for replacing a space, and one for a hyphen, that uses the `gsub()` function in {base} R.  


```{r}

gsub(pattern = " ", replacement = "", x = pc_kimberley_all)
gsub(pattern = "-", replacement = "", x = pc_kimberley_all)

```

Those two steps can be combined by using a single regular expression which replaces either a space or a hyphen:

```{r}

pc_kimberley_clean <- gsub("[ -]", "", pc_kimberley_all)
pc_kimberley_clean


```

There are functions within {stringr} that have the same result, `stringr::str_replace()` and `stringr::str_remove()`.


```{r}

str_replace(string = pc_kimberley_all, 
            pattern = "[ -]", 
            replacement = "")

str_remove(string = pc_kimberley_all, 
           pattern = "[ -]")

```


The second step will be to capitalize the letters. The {stringr} function for this is `str_to_upper`. (There is also parallel `str_to_lower`, `str_to_title`, and `str_to_sentence` functions.)

```{r}
pc_kimberley_clean <- str_to_upper(pc_kimberley_clean)
pc_kimberley_clean
```


If this was in a data frame, these functions can be added to a `mutate()` function, and applied in a pipe sequence, as shown below.

```{r}

# convert string to tibble
pc_kimberley_tbl <- as_tibble(pc_kimberley)

# mutate new values
# --note that creating three separate variables allows for comparisons 
#   as the values change from one step to the next
pc_kimberley_tbl |>
  # extract the postal codes
  mutate(pc_extract = str_extract(value, postalcode)) |>  
  # remove space or hyphen
  mutate(pc_remove = str_remove(pc_extract, "[ -]")) |> 
  # change to upper case
  mutate(pc_upper = str_to_upper(pc_remove))               
  

```



### split and combine strings

Below, we split an address based on comma and space location, using the `str_split()` function.

```{r}
UVic_address <- 
  "Continuing Studies,
   3800 Finnerty Rd, 
   Victoria, BC, 
   V8P 5C2"
  
str_split(UVic_address, ", ")
```



{stringr} contains three useful functions for combining strings. `str_c()` collapses multipe strings into a single string, separated by whatever we specify (the default is nothing, that is to say, `""`.)

Below we will combine the components of an address into a single string, with the components separated by a comma and a space.

```{r}

UVic_address_components <- c(
  "Continuing Studies", 
   "3800 Finnerty Rd", 
   "Victoria", "BC", 
   "V8P 5C2"
  )

str_c(UVic_address_components, collapse = ", ")

```

The `str_flatten_comma()` is a shortcut to the same result:

```{r}
str_flatten_comma(UVic_address_components)

```

Note that `str_flatten` could also be used, but we need to explicitly specify the separator:

```{r}

str_flatten(UVic_address_components, collapse = ", ")

```

These functions also have the argument `last =`, which allows us to specify the final separator:


```{r}

str_flatten(UVic_address_components, collapse = ", ", last = " ")

```


### Example: Bureau of Labor Statistics by NAICS code

In this example, we will use regular expressions to filter a table of employment data published by the US Bureau of Labor Statistics. The table shows the annual employment by industry, for the years 2010 to 2020, in thousands of people.

With this data, it's important to understand the structure of the file, which is rooted in the North American Industry Classification System (NAICS)\index{North American Industry Classification System} \index{NAICS|see{North American Industry Classification System}}. This typology is used in Canada, the United States, and Mexico, grouping companies by their primary output. This consistency allows for reliable and comparable analysis of the economies of the three countries. More information can be found in [@US_NAICS_2022] and [@StatCan_NAICS].  



The NAICS system, and the file we are working with, is hierarchical; higher-level categories subdivide into subsets. Let's examine employment in Mining, Quarrying, and Oil and Gas Extraction sector. 

This sector's coding is as follows:

| Industry | NAICS code |
| ---- | ---- |
| Mining, Quarrying, and Oil and Gas Extraction | 21 |
| * Oil and Gas Extraction | 211 |
| * Mining (except Oil and Gas) | 212 |
|   - Coal Mining | 2121 |
|   - Metal Ore Mining | 2122 |
|   - Nonmetallic Mineral Mining and Quarrying | 2123 |
| * Support Activities for Mining | 213 |


Let's look at the data now. Note that the original CSV file has the data for 2010 through 2020; for this example the years 2019 and 2020 are selected.


```{r, message=FALSE}

bls_employment <-
  read_csv(dpjr::dpjr_data("us_bls_employment_2010-2020.csv")) |>
  select(`Series ID`, `Annual 2019`, `Annual 2020`)

gt(head(bls_employment))

```

There are three "supersectors" in the NAICS system: primary industries, goods-producing industries, and service-producing industries. The "Series ID" variable string starts with "CEU", then the digits in positions 4 and 5 represent the super-sector. The first row, supersector "00", is the aggregation of the entire workforce.

Mining, Quarrying, and Oil and Gas Extraction is a sector within the goods-producing industries, with the NAICS code number 21. This is represented at digits 6 and 7 of the string. 

Mining (NAICS 212) is a subsector, comprised of three industries (at the four-digit level).

The hierarchical structure of the dataframe means that if we simply summed the column we would end up double-counting: the sum of coal, metal ore, and non-metalic mineral industry rows is reported as the employment in Mining (except Oil and Gas).


Let's imagine our assignment is to produce a chart showing the employment in the three subsectors, represented at the three-digit level.

First, let's create a table with all the rows associated with the sector, NAICS 21. 

```{r}

# create NAICS 21 table
bls_employment_naics21 <- bls_employment |> 
  filter(str_detect(`Series ID`, "CEU1021"))

gt(
  bls_employment_naics21
)

```

In the table above, we see the first row has the "Series ID" of CEU1021000001. That long string of zeros shows that this is the sector, NAICS 21. The employment total for each year is the aggregation of the subsectors 211, 212, and 213. As well, this table also shows the industries within 212, starting with  CEU1021210001 (NAICS 2121).

What if we selected only the values that have a zero in the ninth spot?

```{r}
gt(
bls_employment_naics21 |> 
  # remove the ones that have a zero in the 5th spot from the end
  filter(str_detect(`Series ID`, "00001$"))
)
```

That's not quite right—it includes the "21" sector aggregate.

Let's filter out the rows that have a zero after the "21" using an exclamation mark at the beginning of our `str_detect()` function.

```{r}
# NAICS 212
bls_employment_naics21 |> 
  # use the exclamation mark to filter those that don't match
  filter(!str_detect(`Series ID`, "CEU10210"))

```

That doesn't work, because it leaves in the four-digit industries, starting with 2121.

The solution can be found by focussing on the characters that differentiate the levels in the hierarchy. 

Let's add the digits 1 to 9 to the front of the second filter (the 8th character of the string), and leave the next character (the 9th character) as a zero. This should find "CEU1021200001" but omit "CEU1021210001".

```{r}

# NAICS 212
bls_employment_naics21 |> 
  # remove the ones that have a zero in the 5th spot from the end
  # - add any digit from 1-9, leave the following digit as 0
  filter(str_detect(`Series ID`, "[1-9]00001$"))

```

A second option would be to use a negate zero, "[^0]". You'll notice that this uses the carat symbol, "^". Because it is inside the square brackets, it has the effect of negating the named values. This is different behaviour than when it's outside the square brackets and means "start of the string".

```{r}

# NAICS 212
bls_employment_naics21 |> 
  # remove the ones that have a zero in the 6th spot from the end
  # - add any digit that is not a zero
  filter(str_detect(`Series ID`, "[^0]00001$"))

```


And here's one final option, using `str_sub()` to specify the location, inside the `str_detect()` function. 


Let's look at the results of the `str_sub()` function in isolation before we move on. In this code, the function extracts all of the character strings from the 8th and 9th positions in the "Series ID" variable, resulting in strings that are two characters long.

```{r}

# extract the character strings in the 8th & 9th position
str_sub(bls_employment_naics21$`Series ID`, start = 8, end = 9)

```

Starting on the inside of the parentheses, we have the `str_sub()` function to create those two-character strings. Next, the `str_detect()` with a regex that identifies those that have a digit other than zero in the first position, and a zero in the second. The table is then filtered for those rows where that was detected.


```{r}

# NAICS 212
bls_employment_naics21_3digit <- bls_employment_naics21 |>
  # filter those that match the regex in the 8th & 9th position
  filter(str_detect(
    str_sub(`Series ID`, start = 8, end = 9), 
  "[^0][0]"))

bls_employment_naics21_3digit

```


The final step in cleaning this table would be to put it into a tidy structure. Currently it violates the second principle of tidy data, that each observation forms a row. [@tidydata, p.4] In this case, each year should have its own row, rather than a separate column for each.

We can use the {tidyr} [@R-tidyr] package's `pivot_longer()` function for this:

```{r bls_final}

bls_employment_naics21_3digit |> 
  pivot_longer(
    cols = contains("Annual"), 
    names_to = "year", 
    values_to = "employment")

```


### Example: NOC: split code from title

This example is drawn from the Canadian National Occupation Classification (NOC) code \index{National Occupation Code} \index{NOC|see{National Occupation Code}} is in the same column as the code's title.

The table we will use are occupations in British Columbia's accommodation industry.


```{r read_noc, include=FALSE}

noc_table <- read_rds(dpjr::dpjr_data("noc_table.rds"))

gt(head(noc_table))

```

This table has four variables in three columns, each of which has at least one data cleaning challenge.

**`noc_occupation_title`**

This variable combines two values: the three-digit NOC code, and the title of the occupation.

We will use the {tidyr} function `separate_wider_delim()` to split it into two variables. The function gives us the arguments to name the new columns that result from the separation, and the character string that will be used to identify the separation point, in this case ": ". **Don't forget the space!**

This function also has an argument `cols_remove =`. The default is set to `TRUE`, which removes the original column. Set to `FALSE` it retains the original column; we won't use that here but it can be useful.

There are also `separate_wider_()` functions based on fixed-width positions and regular expressions. 


```{r}

noc_table_clean <- noc_table |> 
  tidyr::separate_wider_delim(
    cols = noc_occupation_title, 
    delim = ": ",
    names = c("noc", "occupation_title") 
    )


gt(head(noc_table_clean))

```

**`certification_training_requirements`**

This variable has a character that gets represented as a square at the beginning, and also a space between the square and the first letter. 

Here we can use the {stringr} function `str_remove_all()`. (Remember that the `str_remove()` function removes only the first instance of the character.)

```{r}

noc_table_clean <- noc_table_clean |> 
  mutate(
    certification_training_requirements =
      str_remove_all(certification_training_requirements, " ")
    )

gt(head(noc_table_clean))

```


**`employment`**

This variable should be numbers, but is a character type. This is because

* there are commas used as the thousands separator

* the smallest categories are represented with "-*" to indicate a table note.

In order to clean this, we require three steps:

1. remove commas

2. replace "-*" with NA

3. convert to numeric value

```{r}

noc_table_clean <- noc_table_clean |> 
  # remove commas
  mutate(
    employment =
      str_remove_all(employment, ",")
    ) |> 
  # replace with NA
  mutate(
    employment =
      str_replace_all(employment, "-\\*", replacement = NA_character_)
    ) |> 
  # convert to numeric
  mutate(
    employment =
      as.numeric(employment)
    )
  
gt(head(noc_table_clean))

```





## Creating conditional and calculated variables

We sometimes find ourselves in a situation where we want to recode our existing variables into other categories. An instance of this is when there are a handful of categories that make up the bulk of the total, and summarizing the smaller categories into an "all other" makes the table or plot easier to read. One example of this are the populations of the 13 Canadian provinces and territories: the four most populous provinces (Ontario, Quebec, British Columbia, and Alberta) account for roughly 85% of the country's population...the other 15% of Canadians are spread across nine provinces and territories. We might want to show a table with only five rows, with the smallest nine provinces and territories grouped into a single row.

We can do this with a function in {dplyr}, `case_when()`.

```{r}
canpop <- read_csv(dpjr::dpjr_data("canpop.csv"))
canpop
```

In this first solution, we name the largest provinces separately:

```{r}

canpop |> 
  mutate(pt_grp = case_when(
    # evaluation ~ new value
    province_territory == "Ontario" ~ "Ontario",
    province_territory == "Quebec" ~ "Quebec",
    province_territory == "British Columbia" ~ "British Columbia",
    province_territory == "Alberta" ~ "Alberta",
    # all others get recoded as "other"
    TRUE ~ "other"
  ))

```

But that's a lot of typing. A more streamlined approach is to put the name of our provinces in a list, and then recode our new variable with the value from the original variable "province_territory":

```{r}

canpop |> 
  mutate(pt_grp = case_when(
    province_territory %in% 
      c("Ontario", "Quebec", "British Columbia", "Alberta") ~ 
      province_territory,
    TRUE ~ 
      "other"
  ))

```

We could also use a comparison to create a population threshold, in this case four million:

```{r}

canpop <- canpop |> 
  mutate(pt_grp = case_when(
    population > 4000000 ~ province_territory,
    TRUE ~ "other"
  ))

canpop

```

Whatever the approach we choose, that new variable can be used to group a table:

```{r}

canpop |> 
  group_by(pt_grp) |> 
  summarise(pt_pop = sum(population)) |> 
  arrange(desc(pt_pop))

```


### Classification systems

As we start to create new variables in our data, it is useful to be aware of various classification systems that already exist. That way, you can: 

* save yourself the challenge of coming up with your own classifications for your variables, and

* ensure that your analysis can be compared to other results.

Of course, these classification systems are subject-matter specific. 

As one example, in much social and business data, people's ages are often grouped into 5-year "bins", starting with ages 0–4, 5–9, 10–14, and so on. But not all age classification systems follow this; it will be driven by the specific context. The demographics of a workforce will not require a "0–4" category, while an analysis of school-age children might group them by the ages they attend junior, middle, and secondary school. As a consequence, many demographic tables will provide different data sets with the ages grouped in a variety of ways. (For an example, see [@StatCan_age_of_person].)

And be aware that the elements with a classification system can change over time. One familiar example is that the geographical boundaries of cities and countries of the world have changed. City boundaries change, often with the more populous city expanding to absorb smaller communities within the larger legal entity. (The current City of Toronto is an amalgamation of what were six smaller municipalities.) Some regions have a rather turbulent history of changes in their boundaries, and some countries have been created only to disappear a few years later. 

If you're working in the area of social and economic research, the national and international statistics agencies provide robust and in-depth classification systems, much of which is explicitly designed to allow for international comparisons. These systems cover a wide range of economic and social statistics. Earlier in this chapter there was an example using the North American Industry Classification System (NAICS)\index{North American Industry Classification System}\index{NAICS|see{North American Industry Classification System}}, used in Canada, the United States, and Mexico. [@US_NAICS_2022, @StatCan_NAICS; see also @StatCan_definitions]. A similar classifications system exists for Australia and New Zealand [@ABS_ANZSIC_2006], and the European Union's Eurostat office publishes a standard classification system for the products that are created through mining, manufacturing, and material recovery [@Eurostat_prodcom_2023]. For international statistics, you can start with the classification system maintained by the United Nations Statistics Division [@UNSD_classifications]. In Chapter \@ref(Excel), we saw population statistics reported by the administrative geography of England and Wales.

Following these standardized categories in our own work is advantageous. It provides us with a validated structure, and should it be appropriate, give us access to other data that can be incorporated into our own research.


Your subject area has established classifications; seek them out and use them. Ensuring that your data can be combined with and compared to other datasets will add value to your analysis, and make it easier for future researchers (including future you!) to work with your data. [@White_etal_nine_simple_ways]




## Indicator variables

> To include a categorical variable in a regression, a natural approach is to construct an 'indicator variable' for each category. This allows a separate effect for each level of the category, without assuming any ordering or other structure on the categories. [@Gelman_etal_2014, p.366]

Indicator variables\index{indicator variables} are also known as dummy variables\index{dummy variables|see{indicator variables}} [@Suits_1957] and "one hot" encoding\index{one hot encoding|see{indicator variables}}.

Examples of this approach abound. It can be a useful approach in forecasting methods; Hyndman and Athanasopoulos provide examples where public holidays and days of the week are set as dummy variables [@Hyndman_Athanasopoulos_2021, section 7.4 "Some useful predictors"]

It is also a common approach in social data analysis, where categorical variables are used to code information. In the section on "Discrimination and collider bias" in [@Cunningham_mixtape, pp.106–110], the data are represented in the following way ^[Note: the data in this table is different from what appears in _Causal Inference: The Mixtape_, since the values in the data in that source are randomly generated.]:

```{r ch700_mixtape_data, echo=FALSE}
set.seed(1066) # The rural framework was complete


# code directly from _Causal Mixtape_ https://mixtape.scunning.com/dag.html?panelset=r-code
# except it generates 10 cases, not 10000
tb <- tibble(
  female = ifelse(runif(10)>=0.5,1,0),
  ability = rnorm(10),
  discrimination = female,
  occupation = 1 + 2*ability + 0*female - 2*discrimination + rnorm(10),
  wage = 1 - 1*discrimination + 1*occupation + 2*ability + rnorm(10) 
)

head(tb)
```

In this data, the variable `female` is coded as numeric, so it can be used as part of the regression modeling. 

More commonly, though, this variable will have been captured and saved in one called "gender" or "sex", and often as a character string. The source data for the above table might have originally looked like this:

```{r}

tb2 <- tb |> 
  dplyr::rename(gender = female) |> 
  mutate(gender = case_when(
    gender == 1 ~ "female",
    TRUE ~ "male"
  ))


tb2
```



### `{fastDummies}` 

Because this type of data transformation is common, the package {fastDummies} [@R-fastDummies] has been created, containing the function `dummy_cols()` (or `dummy_columns()` if you prefer extra typing) that creates indicator variables. 

> If you are using the _tidymodels_ pipeline, the {recipes} package [@R-recipes] contains the `step_dummy()` function, which accomplishes much the same result as `fastDummies::dummy_cols()`. The book _Tidy Modeling with R_ by Max Kuhn and Julia Silge [@Kuhn_Silge_2022] is recommended; the creation of indicator variables is covered in Chapter 8 "Feature Engineering with recipes".



```{r}

tb2_indicator <- tb2 |> 
  fastDummies::dummy_cols() 

glimpse(tb2_indicator, width = 65)
```

The variable `gender` has now been mutated into two additional variables, `gender_female` and `gender_male`. The names of the new variables are a concatenation of the original variable name and value, separated by an underscore.

Where the value of `gender` is "female", the value of `gender_female` is assigned as 1, and where `gender` is "male", `gender_female` is 0. This parallels what was in the original data. The opposite is true of `gender_male`; the rows where the value of `gender` is "male" `gender_male` has the value 1.


Multicollinearity can occur in a multiple regression model where multiple indicator variables are included, since they are the inverse of one another. Accordingly, an important set of options in the `dummy_cols()` function revolve around removing all but one of the created variables. The argument `remove_first_dummy = TRUE` does just that; in this case, the `gender_female()` variable does not appear in the final result. This is because the new indicator variables are created in alphabetic order ("female" coming before "male"), and the first is dropped.


```{r}

tb2_indicator <- tb2 |> 
  fastDummies::dummy_cols(remove_first_dummy = TRUE) 

glimpse(tb2_indicator, width = 65)

```

Note that the results of this argument can be controlled through the conversion of character variables to factors. To match the original dataframe, we would retain the variable `gender_female`.

```{r}
# create new tb table with "gender" as factor
tb3 <- tb2 |> 
  mutate(gender = as.factor(gender)) 

levels(tb3$gender)
```

Note that the default order is alphabetical, so "female" remains first. When the argument `remove_first_dummy = TRUE` is applied, we get the same result as before.

```{r}
  
tb3_1 <- tb3 |> 
  fastDummies::dummy_cols(remove_first_dummy = TRUE) 

glimpse(tb3_1, width = 65)

```

By using the `fct_relevel()` function from the {forcats}[@R-forcats] package, we can assign an arbitrary order.

```{r}
# reorder levels 

tb3$gender <- fct_relevel(tb3$gender, "male", "female")

levels(tb3$gender)
```

Rerunning the same code, the `gender_male` variable is dropped.

```{r}
  
tb3_2 <- tb3 |> 
  fastDummies::dummy_cols(remove_first_dummy = TRUE) 

glimpse(tb3_2, width = 65)

```


There is also an option in `fastDummies::dummy_cols()` to `remove_most_frequent_dummy = TRUE`. In our discrimination data, there are more observations coded as "female", so the variable `gender_female` is not included.


```{r}

tb2_frequent <- tb2 |> 
  fastDummies::dummy_cols(remove_most_frequent_dummy = TRUE) 

glimpse(tb2_frequent, width = 65)

```


Another argument in the function is `remove_selected_columns`. The default value is `FALSE`, but if the argument is set to `TRUE`, the source column is excluded from the output. 

```{r}

tb2_select <- tb2 |> 
  fastDummies::dummy_cols(remove_selected_columns = TRUE) 

glimpse(tb2_select, width = 65)

```



Some important things to note about `dummy_cols()` 

* the function will create as many indicator variables as there are values in the character or factor variable

* the function will create indicator variables for all of the character and factor variables in the data, unless otherwise specified.

Let's look at how `dummy_cols()` behaves with two of the variables in {palmerpenguins}: \index{palmerpenguins}  

```{r ch700_dummy_penguins_1}

library(palmerpenguins)

set.seed(1729) #Ramanujan's taxicab number: 1^3 + 12^3 = 9^3 + 10^3

penguin_subset <- penguins |> 
  slice_sample(n = 6) |> 
  dplyr::select(species, island) |> 
  arrange(species)

penguin_subset

```

```{r ch700_dummy_penguins_2}

penguin_subset_1 <- penguin_subset |>  
  fastDummies::dummy_cols()

glimpse(penguin_subset_1, width = 65)
```

First, you will notice that we started with two factor variables, `species` and `island`, we now have indicator variables for both.

And because there are three species of penguins and three islands, we have three indicator variables for both of the original variables.

In the instance where we have multiple character or factor variables in our source data, we can control which variables get the `dummy_cols()` treatment with the option `select_columns`. In the example below, the result provides us with an indicator variable for `species` but not `island`.

In addition, rather than dropping the first created variable, we can specify that the most frequently occurring value be removed with the argument `remove_most_frequent_dummy = TRUE`. Because there are more Gentoo penguins in our sample of the data, the indicator variable `species_Gentoo` is not created.

```{r 700_penguin_dummy}

penguin_subset_2 <- penguin_subset |> 
  fastDummies::dummy_cols(
    select_columns = "species",
    remove_most_frequent_dummy = TRUE)

glimpse(penguin_subset_2, width = 65)

```



## Missing values

\index{missing values}



Frequently, our data will have missing values. Some of this will be gaps in the data, and in other cases it will be due to the structure of the source data file. 

There are a variety of solutions to dealing with missingness in data values, from listwise deletion (removing all rows with missing values) to complex algorithms that apply methods to impute the best estimate of the missing value. There is in-depth literature on this topic (see [@Little_Rubin_2020], [@Gelman_etal_2014, chapter 18], and [@Gelman_Hill_2007, chapter 25]); what is presented here are some solutions to common and relatively simple problems.


### Missing as formatting


A common spreadsheet formatting practice that makes the tables human-readable but less analysis-ready is to have a sub-section heading, often in a separate column. In the example Excel file we will use for this, the sheet "Report" contains data on tuition fees paid by international students in British Columbia, arranged in a wide format. The name individual institution is in the second column (in Excel nomenclature, "B"), and the region ("Economic Development Region") is in the first column. However, the name of the region only appears in the row above the institutions in that region. This makes for a nicely formatted table, but isn't very helpful if we are trying to calculate average tuition fees by region.^[The creators of the file have provided us with a tidy sheet named "Data", and deserve kudos for sharing the data in a variety of layouts. But please be aware that this is approach the exception, not the rule.]

Our first step is to read in the data. Note that the code uses the Excel naming convention to define the rectangular range to be read, and omits the title rows at the top and the notes at the bottom of the sheet.

```{r}

tuition_data <-
  readxl::read_excel(dpjr::dpjr_data("intl_tuition_fees_bc.xlsx"),
                     sheet = "Report",
                     range = "A5:L36")

head(tuition_data)

```

The contents of the file have been read correctly, but there are many "NA" values in the "Economic Development Region" variable. This is due to the manner in which the data was structured in the source Excel file.

The `fill()` function, in the {tidyr} package [@R-tidyr] [\index{{tidyr}}], provides the solution to this. The function replaces a missing value with either the previous or the next value in a variable. In the example here, it will replace all of the "NA" values with the region name above, stopping when it comes to the next non-NA record.

```{r}

tuition_data_filled <- tuition_data |> 
  fill("Economic Development Region")

head(tuition_data_filled)
```

The next step is to use the {tidyr} function `drop_na()` to omit any row that contains an NA value. In this case, we want to drop what were the header rows that contained just the region name in the first column, and no values in the other columns.

These two solutions yield the same result:

```{r}
drop_na(tuition_data_filled)

tuition_data_filled |> 
  drop_na()

```

It's worth noting that the `drop_na()` function provides an argument where we can specify columns. This gives us the flexibility to drop some rows where there is an NA in a specific variable, while keeping other rows that have a value in that variable but not in other variables.


### Missing numeric values


"Missing data are unobserved values that would be meaningful for analysis if observed; in other words, a missing value hides a meaningful value." [@Little_Rubin_2020]

The functions used above for filling missing text values can also be used for missing numeric values. But when dealing with the replacement of numeric values we need to be mindful that the values we use will be incorporated into models and analysis; we need to be clear that the filled values are _estimates_, including sufficient documentation in our methodology section.

Furthermore, there is a great deal of literature on the topic of imputation of missing values. As researchers, we need to be aware of _why_ the values are missing; are we dealing with values that are missing "at completely random", "at random", or "not at random"? Delving into the literature, you'll discover that these terms are precisely defined, and the solutions we adopt to deal with missingness will depend on why the data are missing. [@Little_Rubin_2020]

For the examples presented here, we will assume that you have already given significant thought to these methodological questions. As we discussed earlier in this chapter, _how_ we clean data is influenced by our research questions. As well, we need to consider our cleaning strategy in light of why the raw data are the way they are.

With that preamble, let's move to working with some real data. In this example, the data series is the number of salaried employees in the Accommodation services industry in British Columbia, Canada, for the years 2001 through 2020. The data is collected and published by Statistics Canada, and for some years the data are judged to be "too unreliable to be published". This leaves some gaps in our data. 



```{r read_naics721, echo=FALSE}

naics721_sal <- read_rds(dpjr::dpjr_data("seph_naics721_bc_sal.rds"))

head(naics721_sal)

```

We will employ Exploratory Data Analysis, and plot the data:

```{r}
ggplot(naics721_sal, aes(x = ref_year, y = VALUE)) +
  geom_line() 
```


Our first approach will be to simply drop those years that are missing a value. This might help with some calculations (for example, putting a `na.rm = TRUE` argument in a `mean()` function is doing just that), but it will create a misleading plot—there will be the same distance between the 2009 and 2015 points as there is between 2015 and 2016.

```{r}
  
naics721_sal |> 
  tidyr::drop_na()

```


Alternatively, we can fill the series with the previous valid entry.  

Similar to what we saw above with text, the same `fill()` function can be applied to numeric variables. We will first create a variable `new_value` that will house the same value as our existing variable; the `fill()` function will be applied to `new_value`.


```{r}
# fill down  
naics721_sal_new <- naics721_sal |> 
  mutate(value_fill = VALUE) |> 
  tidyr::fill(value_fill)  # default direction is down

naics721_sal_new

```

Now when we plot the data, the missing points are shown:

```{r}
ggplot(naics721_sal_new, aes(x = ref_year, y = value_fill)) +
  geom_line() 
```


There are a few different options to determine which way to fill; the default is `.direction = "down"`.

```{r, eval=FALSE}

# specified
# down
naics721_sal |> 
  mutate(value_down = VALUE) |> 
  tidyr::fill(value_down, .direction = "down")  # direction is specified as down
# up
naics721_sal |> 
  mutate(value_up = VALUE) |> 
  tidyr::fill(value_up, .direction = "up")  # direction is specified as up


```





### Time series imputation

You may already have realized that the values we filled above are drawn from a time series. Economists, biologists, and many other researchers who work with time series data have built up a large arsenal of tools to work with time series data, including for dealing with missing values. 

For R users, the package {forecast} has time series oriented imputation fill functions. [@R-forecast; @Hyndman_Khandakar_2008]


The `na.interp()` function from {forecast} gives us a much better way to fill the missing values in our time series than simply extending known values. This function, by default, uses as linear interpolation to calculate values between two non-missing values. Here's a simple example:

```{r}
x <- c(1, NA, 3)
x
```

The series from 1 to 3 has an NA value in the middle. The `na.interp()` function fills in the missing value.

```{r}

forecast::na.interp(x)
```

For longer series, a similar process occurs:

```{r}
y <- c(1, NA, NA, 3)
y
```

```{r}
forecast::na.interp(y)

```

You will notice that the function has calculated replacement values that give us a linear series.

For our count of salaried employees, here's what we get when we create a new value `value_new` with this function:

```{r}
naics721_sal <- naics721_sal |> 
  mutate(value_interp = forecast::na.interp(VALUE))

naics721_sal
```

Now when we plot the data, there are no gaps. 

```{r}
ggplot(naics721_sal, aes(x = ref_year, y = value_interp)) +
  geom_line() 
```

The `na.interp()` function also has the capacity to create non-linear models.

Other examples of time series missing value imputation functions can be found in the 
{imputeTS} package [@R-imputeTS], which offers multiple "imputation algorithm implementations along with plotting functions for time series missing data statistics." A fantastic resource for using R for time series analysis, including a section on imputation of missing values, is [@Hyndman_Athanasopoulos_2021].





## Creating labelled factors from numeric variables

In \@ref(importing-stats) we imported statistical software packages such as SPSS, SAS, or Stata and saw the benefits, in certain circumstances, of having labelled variables. These are categorical variables, where there are a limited number of categories. 

In some cases, there might be one or more values assigned to categories we might want to designate as "missing"—a common example can be found in surveys, where respondents are given the option of answering "don't know" and "not applicable". While these responses might be interesting in and of themselves, in some contexts we will want to assign them as missing.

In other cases, numeric values are used to store the categorical responses. 

The Joint Canada/United States Survey of Health (JCUSH)\index{Joint Canada/United States Survey of Health (JCUSH)} file we saw in \@ref(importing-plaintext) importing fixed width files gives us a good example of where categories are stored as numerical variables. For example, the variable "SPJ1_TYP" is either a "1" for responses from the Canadian sample, and a "2" for the sample from the United States. 

We will use this file to create labelled factors from the numeric values in the variables we imported. Here's the code we used previously to read the file:

```{r jcush_txt_read_vars_2}

jcush <- readr::read_fwf(dpjr::dpjr_data("JCUSH.txt"), 
         fwf_cols(
           SAMPLEID = c(1, 12),
           SPJ1_TYP = c(13, 13), 
           GHJ1DHDI = c(32, 32),
           SDJ1GHED = c(502, 502)
           ),
         col_types = list(
           SAMPLEID = col_character()
         ))

head(jcush)
```


The values for these variables are as follows:


| Name | Variable | Code | Value |
| :--- | :---     | :---:  | :---:  |
| SAMPLEID | Household identifier | unique number | &nbsp; |
| SPJ1_TYP | Sample type [country] | 1 | Canada |
| &nbsp; | &nbsp; | 2 | United States |
| GHJ1DHDI | Health Description Index | 0 | Poor |
| &nbsp; | &nbsp; | 1 | Fair |
| &nbsp; | &nbsp; | 2 | Good |
| &nbsp; | &nbsp; | 3 | Very Good |
| &nbsp; | &nbsp; | 4 | Excellent |
| &nbsp; | &nbsp; | 9 | Not Stated |
| SDJ1GHED | Highest level of post-secondary education attained | 1 | Less than high school |
| &nbsp; | &nbsp; | 2 | High school degree or equivalent (GED) |
| &nbsp; | &nbsp; | 3 | Trades Cert, Voc. Sch./Comm.Col./CEGEP |
| &nbsp; | &nbsp; | 4 | Univ or Coll. Cert. incl. below Bach. |
| &nbsp; | &nbsp; | 9 | Not Stated |



The variable "GHJ1DHDI" has the respondent's self-assessment of their overall health. Here's a summary table of the responses:

```{r}
jcush |> 
  group_by(GHJ1DHDI) |> 
  tally()

```

But it would be much more helpful and efficient if we attach the value labels to the dataframe.

One strategy is to create a new factor variable, that uses the value description instead of the numeric representation. For this, we will use the `fct_recode()` function from the {forcats} package[@R-forcats].

```{r}
library(forcats)

jcush_forcats <- jcush |>
  # 1st mutate a new factor variable with the original values
  mutate(health_desc_fct = as.factor(GHJ1DHDI)) |>
  # 2nd recode the values
  mutate(
    health_desc_fct = fct_recode(
      health_desc_fct,
      "Poor" = "0",
      "Fair" = "1",
      "Good" = "2",
      "Very Good" = "3",
      "Excellent" = "4",
      NULL = "9"
    )
  )


```

The `levels()` function allows us to inspect the labels. Note that because we specified the original value of "9" as `NULL`, it does not appear in the list.

```{r}
levels(jcush_forcats$health_desc_fct)
```

When we tally the results by our new variable, the "GHJ1DHDI" records with the value 9 are shown as "NA".

```{r}
jcush_forcats |> 
  group_by(GHJ1DHDI, health_desc_fct) |> 
  tally()

```


Another option is provided in the {haven} package [@R-haven].

```{r}
library(haven)

jcush_haven <- jcush |>
  mutate(health_desc_lab = labelled(
    GHJ1DHDI,
    c(
      "Poor" = 0,
      "Fair" = 1,
      "Good" = 2,
      "Very Good" = 3,
      "Excellent" = 4,
      "Not Stated" = 9
    )
  ))

```

Note that our new variable is a "S3: haven_labelled" class, and the `ls.str()` function shows it as "dbl+lbl":

```{r}
ls.str(jcush_haven)
```


The `levels()` function can be used to inspect the result of this mutate.

```{r}

head(jcush_haven$health_desc_lab)

```

A third option is to use the functions within the {labelled} package [@R-labelled]. 

```{r}
jcush_labelled <- jcush |>
  mutate(health_desc_lab = labelled(
    GHJ1DHDI,
    c(
      "Poor" = 0,
      "Fair" = 1,
      "Good" = 2,
      "Very Good" = 3,
      "Excellent" = 4,
      "Not Stated" = 9
    )
  ))

```

To view a single variable:

```{r}

head(jcush_labelled$health_desc_lab)

```




This package also allows for a descriptive name of the variable to be appended with the `var_lab()` function.


